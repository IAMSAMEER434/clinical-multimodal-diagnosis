{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000b7ed6b18b4cd5965a3f4bfa3f0819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_499fdb0cb4b1440fb1dd5d1f1cf33b95",
              "IPY_MODEL_b222ca255cba4f84a35a524c4132997c",
              "IPY_MODEL_e9fb62a8846640e8b7c1f7f4093c304f"
            ],
            "layout": "IPY_MODEL_c4720fb0243a4686875daf3bf8829842"
          }
        },
        "499fdb0cb4b1440fb1dd5d1f1cf33b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e70b3b5843f649fca1ce069617903352",
            "placeholder": "​",
            "style": "IPY_MODEL_6a53961a2a1344adab5d5595254d0fa6",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "b222ca255cba4f84a35a524c4132997c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c9c64281a14a78b9bfdc33e58887d7",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b521ea3c534b45aea969739a69484903",
            "value": 159
          }
        },
        "e9fb62a8846640e8b7c1f7f4093c304f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a22c7956a044d2ba7f3a18d47311ab",
            "placeholder": "​",
            "style": "IPY_MODEL_c839c6c2477c4127a1a08ede0f2cbc3f",
            "value": " 159/159 [00:00&lt;00:00, 6.85kB/s]"
          }
        },
        "c4720fb0243a4686875daf3bf8829842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e70b3b5843f649fca1ce069617903352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a53961a2a1344adab5d5595254d0fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37c9c64281a14a78b9bfdc33e58887d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b521ea3c534b45aea969739a69484903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21a22c7956a044d2ba7f3a18d47311ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c839c6c2477c4127a1a08ede0f2cbc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c80474081bc74696be1ed139c31d1001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f51172f1458c47ecb87d26f33a74039b",
              "IPY_MODEL_b99501112ad04bfd861ff57c10156102",
              "IPY_MODEL_19d9e3688a434b169e5b9a4e1e85deec"
            ],
            "layout": "IPY_MODEL_2ef87c095d5945e79dc803aaf50786f7"
          }
        },
        "f51172f1458c47ecb87d26f33a74039b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5114576c67974651925680e71fca6fc1",
            "placeholder": "​",
            "style": "IPY_MODEL_88b03f8a4c044e4d8ac9c696c2f62d31",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b99501112ad04bfd861ff57c10156102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be1a86513264986b32ea0c6a95e2c6f",
            "max": 163,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a4584b85ec746a5881341ac59a6ed19",
            "value": 163
          }
        },
        "19d9e3688a434b169e5b9a4e1e85deec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4385f3a0d24ca395640d743e98db9e",
            "placeholder": "​",
            "style": "IPY_MODEL_45818fb7cdae4107a5b5fcd745cf90bf",
            "value": " 163/163 [00:00&lt;00:00, 9.47kB/s]"
          }
        },
        "2ef87c095d5945e79dc803aaf50786f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5114576c67974651925680e71fca6fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b03f8a4c044e4d8ac9c696c2f62d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8be1a86513264986b32ea0c6a95e2c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4584b85ec746a5881341ac59a6ed19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e4385f3a0d24ca395640d743e98db9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45818fb7cdae4107a5b5fcd745cf90bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "551a043580cd4ecf88cdf99549aac48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aebb405911d64b47b57df92d64d736d2",
              "IPY_MODEL_88035f65081a48328bf3614ad2593196",
              "IPY_MODEL_18d634810d81451d9b1d582a83c3b815"
            ],
            "layout": "IPY_MODEL_dcd12aea70bf433f8830608f00c80c5e"
          }
        },
        "aebb405911d64b47b57df92d64d736d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01cb7c122c324ecb86c3a4e0e05cbf25",
            "placeholder": "​",
            "style": "IPY_MODEL_9889acb39b614e2ea55255979142a3c8",
            "value": "config.json: "
          }
        },
        "88035f65081a48328bf3614ad2593196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73ea0a9d3196409a9d9c93ae5085645f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9569cab1fa94454c93405eda2f91a537",
            "value": 1
          }
        },
        "18d634810d81451d9b1d582a83c3b815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e49bad0a44414f8d9c31f8367b26e199",
            "placeholder": "​",
            "style": "IPY_MODEL_14488979a9fc4dd7a81dfa89d1be98a2",
            "value": " 1.60k/? [00:00&lt;00:00, 48.5kB/s]"
          }
        },
        "dcd12aea70bf433f8830608f00c80c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01cb7c122c324ecb86c3a4e0e05cbf25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9889acb39b614e2ea55255979142a3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73ea0a9d3196409a9d9c93ae5085645f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9569cab1fa94454c93405eda2f91a537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e49bad0a44414f8d9c31f8367b26e199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14488979a9fc4dd7a81dfa89d1be98a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a124ec4bb52846909ba8036a7d388aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b456c71cc6440a7a493003e30358025",
              "IPY_MODEL_7c531171dea24bc388a872f00a9a43e0",
              "IPY_MODEL_c116abc17aef4c93abbf636edfc287bb"
            ],
            "layout": "IPY_MODEL_042787ef4fff416982ce35df48617d35"
          }
        },
        "6b456c71cc6440a7a493003e30358025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efab17185f3c414d9433a2db21b6d3fc",
            "placeholder": "​",
            "style": "IPY_MODEL_5bf81fd741eb4e08af65248093190c01",
            "value": "vocab.json: 100%"
          }
        },
        "7c531171dea24bc388a872f00a9a43e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaeec4ce2fdf4a6aabe17a70738e8e31",
            "max": 291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eed07d2fa94446e4bf29ed8330025d3e",
            "value": 291
          }
        },
        "c116abc17aef4c93abbf636edfc287bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66780729d4ea4681b1662e9084dd72e1",
            "placeholder": "​",
            "style": "IPY_MODEL_58d37d8df3204979b7b5db5a8edecc1e",
            "value": " 291/291 [00:00&lt;00:00, 13.8kB/s]"
          }
        },
        "042787ef4fff416982ce35df48617d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efab17185f3c414d9433a2db21b6d3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf81fd741eb4e08af65248093190c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaeec4ce2fdf4a6aabe17a70738e8e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed07d2fa94446e4bf29ed8330025d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66780729d4ea4681b1662e9084dd72e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d37d8df3204979b7b5db5a8edecc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f8757f77547427fa8e96d79061f40ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb583c5c696642698dbde6f41d16e408",
              "IPY_MODEL_43cf3b7e5f9449049cd708aa759cf2de",
              "IPY_MODEL_b5b8e6e5e2024b3a8ac306c18c5b62e2"
            ],
            "layout": "IPY_MODEL_f3bbd149470e4a83a62a03a83b94c790"
          }
        },
        "cb583c5c696642698dbde6f41d16e408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c8cdc5d7534a09bd99911ec9eec08a",
            "placeholder": "​",
            "style": "IPY_MODEL_adefdacaf89b4eaf8b4e77540fa170d1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "43cf3b7e5f9449049cd708aa759cf2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_214fc3c412c440b2aebf78ee966b2c31",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2326a81876247acab62939cc08b0a3a",
            "value": 85
          }
        },
        "b5b8e6e5e2024b3a8ac306c18c5b62e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aef61c62cf34170b0fa8d491283f161",
            "placeholder": "​",
            "style": "IPY_MODEL_3655aa918e724ff0b821a550cfd54e1a",
            "value": " 85.0/85.0 [00:00&lt;00:00, 4.25kB/s]"
          }
        },
        "f3bbd149470e4a83a62a03a83b94c790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c8cdc5d7534a09bd99911ec9eec08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adefdacaf89b4eaf8b4e77540fa170d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "214fc3c412c440b2aebf78ee966b2c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2326a81876247acab62939cc08b0a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4aef61c62cf34170b0fa8d491283f161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3655aa918e724ff0b821a550cfd54e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c10f4b51f4b4376ad2c07ace615da9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff3a637391c74531b59916d5fed44a6d",
              "IPY_MODEL_9f12d948bde949b786d3b31f0afb9335",
              "IPY_MODEL_2acdbe61b72a42728d0022c364478c9d"
            ],
            "layout": "IPY_MODEL_fbd6f76348994f0e9f615c4fd69406f4"
          }
        },
        "ff3a637391c74531b59916d5fed44a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_225c95164b5d4fc0a3afa8b54e454b2b",
            "placeholder": "​",
            "style": "IPY_MODEL_a99e95c370834aafbd4dc4d73ea17887",
            "value": "model.safetensors: 100%"
          }
        },
        "9f12d948bde949b786d3b31f0afb9335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f107935d3a7a43d68ee8d14b595b30ee",
            "max": 377607901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96cecf513e574245ae080240d60772a3",
            "value": 377607901
          }
        },
        "2acdbe61b72a42728d0022c364478c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ca26957e9e140f48c80d863040f7603",
            "placeholder": "​",
            "style": "IPY_MODEL_477e72e716924c2e8cff04a3a79025f7",
            "value": " 378M/378M [00:03&lt;00:00, 240MB/s]"
          }
        },
        "fbd6f76348994f0e9f615c4fd69406f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "225c95164b5d4fc0a3afa8b54e454b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99e95c370834aafbd4dc4d73ea17887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f107935d3a7a43d68ee8d14b595b30ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cecf513e574245ae080240d60772a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ca26957e9e140f48c80d863040f7603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477e72e716924c2e8cff04a3a79025f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "324315a4d7c7459d923027a4df3a14d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43e87244a5ba4cf7a11f4b25ea7dbf2d",
              "IPY_MODEL_0ea87743244b4da5868bbf076c3c466e",
              "IPY_MODEL_af82d3f219734128b55cb5f97a7d6616"
            ],
            "layout": "IPY_MODEL_7b141bd3e791448989e76ce36ed79609"
          }
        },
        "43e87244a5ba4cf7a11f4b25ea7dbf2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6d8b490c9f54dd0bb12ae9037c2eec4",
            "placeholder": "​",
            "style": "IPY_MODEL_88537aa27aea457c9c1f78e7bd07e345",
            "value": "model.safetensors: 100%"
          }
        },
        "0ea87743244b4da5868bbf076c3c466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c944e47eab5f450ba86dd24972494d4e",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecb8b88031cb47b5a88d1e17873b5968",
            "value": 49335454
          }
        },
        "af82d3f219734128b55cb5f97a7d6616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d9a7dd261524bffb6f93abcd9deb298",
            "placeholder": "​",
            "style": "IPY_MODEL_36083f443cd849d69507f680f71d7472",
            "value": " 49.3M/49.3M [00:05&lt;00:00, 8.66MB/s]"
          }
        },
        "7b141bd3e791448989e76ce36ed79609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d8b490c9f54dd0bb12ae9037c2eec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88537aa27aea457c9c1f78e7bd07e345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c944e47eab5f450ba86dd24972494d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecb8b88031cb47b5a88d1e17873b5968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d9a7dd261524bffb6f93abcd9deb298": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36083f443cd849d69507f680f71d7472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZStceIKXfg8",
        "outputId": "a319010c-648a-4b8e-9c1a-29d8f8ef79e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset extracted to: /content/multimodal_audio/audio\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to your uploaded ZIP file (make sure it's in /content)\n",
        "zip_path = \"/content/audio_files.zip\"  # << change if path differs\n",
        "\n",
        "# Output folder where images will be extracted\n",
        "extract_dir = Path(\"/content/multimodal_audio/audio\")\n",
        "\n",
        "# Create directory if it doesn’t exist\n",
        "extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"✅ Dataset extracted to: {extract_dir.resolve()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to your uploaded ZIP file (make sure it's in /content)\n",
        "zip_path = \"/content/Multimodal Captioning Dataset.zip\"  # << change if path differs\n",
        "\n",
        "# Output folder where images will be extracted\n",
        "extract_dir = Path(\"/content/Multimodal_Captioning_Dataset\")\n",
        "\n",
        "# Create directory if it doesn’t exist\n",
        "extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"✅ Dataset extracted to: {extract_dir.resolve()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-CoPcUQZTxj",
        "outputId": "91b70565-7537-4ab1-cd61-bd81485dfbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset extracted to: /content/Multimodal_Captioning_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Af05DypTZVLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: Tri-modal training (5-fold) -> saves logits, probs, y_true, fold_ids\n",
        "# Paste entire block into one Colab cell and run.\n",
        "\n",
        "!pip install -q timm transformers librosa soundfile scikit-learn\n",
        "\n",
        "import os, gc, random, time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import timm\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "CSV_PATH = \"/content/clips_with_audio_clean2.csv\"   # adjust if needed\n",
        "TEXT_COL = \"Updated_Question\"                      # or \"Question_summ\"\n",
        "IMG_COL  = \"Image_path_final\"                      # or \"Image_path\"\n",
        "AUDIO_COL= \"audio_path\"\n",
        "LABEL_COL= \"probable_classes\"\n",
        "\n",
        "EMB_DIR = \"/content/wav2vec_tri_embs\"\n",
        "os.makedirs(EMB_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = 300\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 20\n",
        "FOLDS = 5\n",
        "SEED = 42\n",
        "TARGET_SR = 16000\n",
        "AUDIO_SEC = 3\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -------------------- LOAD CSV --------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Original rows:\", len(df))\n",
        "\n",
        "# basic filter: need text,img,audio,label\n",
        "mask = df.get(TEXT_COL).notna() & df.get(IMG_COL).notna() & df.get(AUDIO_COL).notna() & df.get(LABEL_COL).notna()\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Rows after filter:\", len(df))\n",
        "\n",
        "# -------------------- LABELS -> multi-hot Y --------------------\n",
        "raw_labels = df[LABEL_COL].astype(str).tolist()\n",
        "all_labels = sorted({lbl.strip() for row in raw_labels for lbl in row.split(\",\") if lbl.strip()})\n",
        "label_to_idx = {lab:i for i,lab in enumerate(all_labels)}\n",
        "print(\"Num distinct labels:\", len(all_labels))\n",
        "print(\"Example labels:\", all_labels[:12])\n",
        "\n",
        "num_labels = len(all_labels)\n",
        "Y = np.zeros((len(df), num_labels), dtype=np.float32)\n",
        "for i, r in enumerate(raw_labels):\n",
        "    for tok in r.split(\",\"):\n",
        "        tok = tok.strip()\n",
        "        if tok in label_to_idx:\n",
        "            Y[i, label_to_idx[tok]] = 1.0\n",
        "\n",
        "df[\"row_id\"] = np.arange(len(df))\n",
        "print(\"Multi-label matrix shape:\", Y.shape)\n",
        "\n",
        "# save y_true for later CI computation\n",
        "np.save(\"/content/y_true.npy\", Y)\n",
        "print(\"Saved /content/y_true.npy\")\n",
        "\n",
        "# -------------------- TF-IDF (text features) --------------------\n",
        "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
        "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1,2), min_df=2)\n",
        "X_text_sparse = tfidf.fit_transform(texts)\n",
        "X_text = X_text_sparse.toarray().astype(np.float32)   # dense for convenience\n",
        "text_dim = X_text.shape[1]\n",
        "print(\"TF-IDF dim:\", text_dim)\n",
        "\n",
        "# -------------------- wav2vec2: precompute audio embeddings --------------------\n",
        "print(\"Loading wav2vec2 (this may download ~350MB)...\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device).eval()\n",
        "emb_dim = wav2vec.config.hidden_size\n",
        "print(\"wav2vec emb dim:\", emb_dim)\n",
        "\n",
        "def extract_wav2vec_emb(row_id, audio_path):\n",
        "    out_path = os.path.join(EMB_DIR, f\"emb_{row_id}.npy\")\n",
        "    if os.path.exists(out_path):\n",
        "        return\n",
        "    try:\n",
        "        y, sr = sf.read(audio_path, dtype=\"float32\")\n",
        "    except Exception as e:\n",
        "        # fallback to librosa\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=None)\n",
        "            y = y.astype(\"float32\")\n",
        "        except Exception as e2:\n",
        "            print(f\"[WARN] audio load failed for {audio_path}: {e} / {e2}\")\n",
        "            np.save(out_path, np.zeros(emb_dim, dtype=np.float32))\n",
        "            return\n",
        "    if y is None:\n",
        "        np.save(out_path, np.zeros(emb_dim, dtype=np.float32))\n",
        "        return\n",
        "    if y.ndim > 1:\n",
        "        y = np.mean(y, axis=1)\n",
        "    if sr != TARGET_SR:\n",
        "        try:\n",
        "            y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
        "            sr = TARGET_SR\n",
        "        except Exception:\n",
        "            # if resample fails, pad/trim to something\n",
        "            pass\n",
        "    needed = TARGET_SR * AUDIO_SEC\n",
        "    if len(y) < needed:\n",
        "        y = np.pad(y, (0, needed - len(y)))\n",
        "    else:\n",
        "        y = y[:needed]\n",
        "    with torch.no_grad():\n",
        "        inp = processor(y, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inp[\"input_values\"].to(device)\n",
        "        out = wav2vec(input_values)\n",
        "        hid = out.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy().astype(np.float32)\n",
        "    np.save(out_path, hid)\n",
        "\n",
        "print(\"Precomputing wav2vec embeddings...\")\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    extract_wav2vec_emb(int(row.row_id), row[AUDIO_COL])\n",
        "print(\"Wav2vec embeddings saved to\", EMB_DIR)\n",
        "\n",
        "# -------------------- Image transforms --------------------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.ColorJitter(0.12,0.12,0.08,0.04),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# -------------------- Dataset --------------------\n",
        "class TriModalDataset(Dataset):\n",
        "    def __init__(self, df_sub, X_text, Y_full, transform, emb_dir):\n",
        "        self.df_sub = df_sub.reset_index(drop=True)\n",
        "        self.X_text = X_text\n",
        "        self.Y = Y_full\n",
        "        self.transform = transform\n",
        "        self.emb_dir = emb_dir\n",
        "    def __len__(self): return len(self.df_sub)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df_sub.iloc[idx]\n",
        "        rid = int(row[\"row_id\"])\n",
        "        # image\n",
        "        try:\n",
        "            img = Image.open(row[IMG_COL]).convert(\"RGB\")\n",
        "            img = self.transform(img)\n",
        "        except Exception:\n",
        "            img = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
        "        # text dense\n",
        "        x_text = torch.from_numpy(self.X_text[rid])\n",
        "        # audio emb\n",
        "        emb_path = os.path.join(self.emb_dir, f\"emb_{rid}.npy\")\n",
        "        if os.path.exists(emb_path):\n",
        "            emb = np.load(emb_path).astype(np.float32)\n",
        "        else:\n",
        "            emb = np.zeros(emb_dim, dtype=np.float32)\n",
        "        emb = torch.from_numpy(emb)\n",
        "        y = torch.from_numpy(self.Y[rid])\n",
        "        return img, x_text, emb, y\n",
        "\n",
        "# -------------------- Model --------------------\n",
        "class ImgEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # create model with num_classes=0 to return features\n",
        "        self.backbone = timm.create_model(\"efficientnet_b3\", pretrained=True, num_classes=0)\n",
        "        # determine output dim by running a dummy through the backbone\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE)\n",
        "            feat = self.backbone(dummy)\n",
        "            self.out_dim = feat.shape[1]\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "class TriModalNet(nn.Module):\n",
        "    def __init__(self, text_dim, audio_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.img_enc = ImgEncoder()\n",
        "        self.text_proj = nn.Sequential(nn.Linear(text_dim, 512), nn.ReLU(), nn.LayerNorm(512), nn.Dropout(0.3))\n",
        "        self.audio_proj = nn.Sequential(nn.Linear(audio_dim, 512), nn.ReLU(), nn.LayerNorm(512), nn.Dropout(0.3))\n",
        "        fusion_dim = self.img_enc.out_dim + 512 + 512\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 1024), nn.ReLU(), nn.LayerNorm(1024), nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    def forward(self, img, txt, aud):\n",
        "        img_feat = self.img_enc(img)\n",
        "        txt_feat = self.text_proj(txt)\n",
        "        aud_feat = self.audio_proj(aud)\n",
        "        x = torch.cat([img_feat, txt_feat, aud_feat], dim=1)\n",
        "        return self.head(x)\n",
        "\n",
        "# -------------------- Metrics helper --------------------\n",
        "def compute_micro_macro_from_probs(y_true, y_prob, thr=0.5, eps=1e-9):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    # micro\n",
        "    tp = ((y_true==1) & (y_pred==1)).sum()\n",
        "    fp = ((y_true==0) & (y_pred==1)).sum()\n",
        "    fn = ((y_true==1) & (y_pred==0)).sum()\n",
        "    micro = 2*tp / (2*tp + fp + fn + eps)\n",
        "    # macro: average per-class f1 (skip classes with zero support)\n",
        "    C = y_true.shape[1]\n",
        "    f1s = []\n",
        "    for c in range(C):\n",
        "        yt = y_true[:,c]\n",
        "        yp = y_pred[:,c]\n",
        "        tp_c = ((yt==1) & (yp==1)).sum()\n",
        "        fp_c = ((yt==0) & (yp==1)).sum()\n",
        "        fn_c = ((yt==1) & (yp==0)).sum()\n",
        "        den = (2*tp_c + fp_c + fn_c + eps)\n",
        "        if (yt.sum() == 0):\n",
        "            continue\n",
        "        f1s.append(2*tp_c / den)\n",
        "    macro = float(np.mean(f1s)) if len(f1s)>0 else 0.0\n",
        "    return float(micro), float(macro)\n",
        "\n",
        "# -------------------- 5-Fold training --------------------\n",
        "primary = Y.argmax(axis=1)  # simple stratification\n",
        "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "N = len(df)\n",
        "oof_logits = np.zeros((N, num_labels), dtype=np.float32)\n",
        "fold_ids = np.zeros(N, dtype=np.int32)\n",
        "\n",
        "fold_iter = list(skf.split(np.arange(N), primary))\n",
        "for fold, (tr_idx, vl_idx) in enumerate(fold_iter, start=1):\n",
        "    print(f\"\\n=== FOLD {fold}/{FOLDS} ===\")\n",
        "    fold_ids[vl_idx] = fold\n",
        "\n",
        "    tr_df, vl_df = df.iloc[tr_idx].reset_index(drop=True), df.iloc[vl_idx].reset_index(drop=True)\n",
        "\n",
        "    tr_ds = TriModalDataset(tr_df, X_text, Y, train_tf, EMB_DIR)\n",
        "    vl_ds = TriModalDataset(vl_df, X_text, Y, eval_tf, EMB_DIR)\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    vl_loader = DataLoader(vl_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    # build model\n",
        "    model = TriModalNet(text_dim=text_dim, audio_dim=emb_dim, num_classes=num_labels).to(device)\n",
        "\n",
        "    # compute pos_weight from training split\n",
        "    y_tr = Y[tr_idx]\n",
        "    pos = y_tr.sum(axis=0) + 1e-6\n",
        "    neg = (y_tr.shape[0] - pos) + 1e-6\n",
        "    pos_weight = torch.tensor(neg/pos, dtype=torch.float32, device=device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "    best_micro = 0.0\n",
        "    best_path = f\"/content/tri_best_fold{fold}.pth\"\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, txts, embs, ys in tr_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            txts = txts.to(device)\n",
        "            embs = embs.to(device)\n",
        "            ys = ys.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs, txts, embs)\n",
        "            loss = criterion(logits, ys)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # validation per epoch\n",
        "        model.eval()\n",
        "        val_logits = []\n",
        "        val_trues = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, txts, embs, ys in vl_loader:\n",
        "                imgs = imgs.to(device); txts = txts.to(device); embs = embs.to(device)\n",
        "                logits = model(imgs, txts, embs).cpu().numpy()\n",
        "                val_logits.append(logits)\n",
        "                val_trues.append(ys.numpy())\n",
        "        val_logits = np.vstack(val_logits)\n",
        "        val_trues = np.vstack(val_trues)\n",
        "        val_probs = 1.0 / (1.0 + np.exp(-val_logits))\n",
        "        micro, macro = compute_micro_macro_from_probs(val_trues, val_probs, thr=0.5)\n",
        "        print(f\"Fold {fold} Epoch {epoch}: Loss {running_loss/len(tr_loader):.4f} | micro-F1 = {micro:.4f} | macro-F1 = {macro:.4f}\")\n",
        "        if micro > best_micro + 1e-6:\n",
        "            best_micro = micro\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f\"  -> saved best model for fold {fold} (micro {best_micro:.4f})\")\n",
        "\n",
        "    # load best and compute OOF logits (raw)\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, txts, embs, ys in vl_loader:\n",
        "            imgs = imgs.to(device); txts = txts.to(device); embs = embs.to(device)\n",
        "            logits = model(imgs, txts, embs).cpu().numpy()\n",
        "            all_logits.append(logits)\n",
        "    all_logits = np.vstack(all_logits)\n",
        "    if all_logits.shape[0] != len(vl_idx):\n",
        "        # safety: if batch sizes or ordering mismatch, resize accordingly\n",
        "        print(f\"[WARN] fold {fold} vl len mismatch {all_logits.shape[0]} vs {len(vl_idx)}; trimming/padding as needed\")\n",
        "        n_min = min(all_logits.shape[0], len(vl_idx))\n",
        "        oof_logits[vl_idx[:n_min]] = all_logits[:n_min]\n",
        "        if all_logits.shape[0] < len(vl_idx):\n",
        "            # pad remaining with zeros\n",
        "            oof_logits[vl_idx[all_logits.shape[0]:]] = 0.0\n",
        "    else:\n",
        "        oof_logits[vl_idx] = all_logits\n",
        "\n",
        "    print(f\"Fold {fold} done. Best micro-F1 on val: {best_micro:.4f}\")\n",
        "    # free memory\n",
        "    del model, tr_loader, vl_loader, tr_ds, vl_ds, optimizer\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# save fold ids and logits and probs\n",
        "np.save(\"/content/trimodal_oof_logits.npy\", oof_logits)\n",
        "np.save(\"/content/trimodal_oof_probs.npy\", (1.0/(1.0+np.exp(-oof_logits))))\n",
        "np.save(\"/content/fold_ids.npy\", fold_ids)\n",
        "print(\"Saved: /content/trimodal_oof_logits.npy, /content/trimodal_oof_probs.npy, /content/fold_ids.npy\")\n",
        "\n",
        "# Global threshold search (micro-F1)\n",
        "best_thr, best_micro = 0.5, 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 11):\n",
        "    probs = 1.0/(1.0+np.exp(-oof_logits))\n",
        "    micro, macro = compute_micro_macro_from_probs(Y, probs, thr=thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro = micro; best_thr = thr\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {best_micro:.4f}\")\n",
        "\n",
        "final_micro, final_macro = compute_micro_macro_from_probs(Y, 1.0/(1.0+np.exp(-oof_logits)), thr=best_thr)\n",
        "print(f\"Final OOF micro-F1: {final_micro:.4f} | macro-F1: {final_macro:.4f}\")\n",
        "\n",
        "# done\n",
        "print(\"All done. Files saved in /content/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "000b7ed6b18b4cd5965a3f4bfa3f0819",
            "499fdb0cb4b1440fb1dd5d1f1cf33b95",
            "b222ca255cba4f84a35a524c4132997c",
            "e9fb62a8846640e8b7c1f7f4093c304f",
            "c4720fb0243a4686875daf3bf8829842",
            "e70b3b5843f649fca1ce069617903352",
            "6a53961a2a1344adab5d5595254d0fa6",
            "37c9c64281a14a78b9bfdc33e58887d7",
            "b521ea3c534b45aea969739a69484903",
            "21a22c7956a044d2ba7f3a18d47311ab",
            "c839c6c2477c4127a1a08ede0f2cbc3f",
            "c80474081bc74696be1ed139c31d1001",
            "f51172f1458c47ecb87d26f33a74039b",
            "b99501112ad04bfd861ff57c10156102",
            "19d9e3688a434b169e5b9a4e1e85deec",
            "2ef87c095d5945e79dc803aaf50786f7",
            "5114576c67974651925680e71fca6fc1",
            "88b03f8a4c044e4d8ac9c696c2f62d31",
            "8be1a86513264986b32ea0c6a95e2c6f",
            "7a4584b85ec746a5881341ac59a6ed19",
            "8e4385f3a0d24ca395640d743e98db9e",
            "45818fb7cdae4107a5b5fcd745cf90bf",
            "551a043580cd4ecf88cdf99549aac48b",
            "aebb405911d64b47b57df92d64d736d2",
            "88035f65081a48328bf3614ad2593196",
            "18d634810d81451d9b1d582a83c3b815",
            "dcd12aea70bf433f8830608f00c80c5e",
            "01cb7c122c324ecb86c3a4e0e05cbf25",
            "9889acb39b614e2ea55255979142a3c8",
            "73ea0a9d3196409a9d9c93ae5085645f",
            "9569cab1fa94454c93405eda2f91a537",
            "e49bad0a44414f8d9c31f8367b26e199",
            "14488979a9fc4dd7a81dfa89d1be98a2",
            "a124ec4bb52846909ba8036a7d388aaf",
            "6b456c71cc6440a7a493003e30358025",
            "7c531171dea24bc388a872f00a9a43e0",
            "c116abc17aef4c93abbf636edfc287bb",
            "042787ef4fff416982ce35df48617d35",
            "efab17185f3c414d9433a2db21b6d3fc",
            "5bf81fd741eb4e08af65248093190c01",
            "eaeec4ce2fdf4a6aabe17a70738e8e31",
            "eed07d2fa94446e4bf29ed8330025d3e",
            "66780729d4ea4681b1662e9084dd72e1",
            "58d37d8df3204979b7b5db5a8edecc1e",
            "6f8757f77547427fa8e96d79061f40ba",
            "cb583c5c696642698dbde6f41d16e408",
            "43cf3b7e5f9449049cd708aa759cf2de",
            "b5b8e6e5e2024b3a8ac306c18c5b62e2",
            "f3bbd149470e4a83a62a03a83b94c790",
            "35c8cdc5d7534a09bd99911ec9eec08a",
            "adefdacaf89b4eaf8b4e77540fa170d1",
            "214fc3c412c440b2aebf78ee966b2c31",
            "a2326a81876247acab62939cc08b0a3a",
            "4aef61c62cf34170b0fa8d491283f161",
            "3655aa918e724ff0b821a550cfd54e1a",
            "0c10f4b51f4b4376ad2c07ace615da9c",
            "ff3a637391c74531b59916d5fed44a6d",
            "9f12d948bde949b786d3b31f0afb9335",
            "2acdbe61b72a42728d0022c364478c9d",
            "fbd6f76348994f0e9f615c4fd69406f4",
            "225c95164b5d4fc0a3afa8b54e454b2b",
            "a99e95c370834aafbd4dc4d73ea17887",
            "f107935d3a7a43d68ee8d14b595b30ee",
            "96cecf513e574245ae080240d60772a3",
            "8ca26957e9e140f48c80d863040f7603",
            "477e72e716924c2e8cff04a3a79025f7",
            "324315a4d7c7459d923027a4df3a14d1",
            "43e87244a5ba4cf7a11f4b25ea7dbf2d",
            "0ea87743244b4da5868bbf076c3c466e",
            "af82d3f219734128b55cb5f97a7d6616",
            "7b141bd3e791448989e76ce36ed79609",
            "d6d8b490c9f54dd0bb12ae9037c2eec4",
            "88537aa27aea457c9c1f78e7bd07e345",
            "c944e47eab5f450ba86dd24972494d4e",
            "ecb8b88031cb47b5a88d1e17873b5968",
            "9d9a7dd261524bffb6f93abcd9deb298",
            "36083f443cd849d69507f680f71d7472"
          ]
        },
        "id": "qACaygKDfHBT",
        "outputId": "5b8d2963-5ba6-4463-e0db-80b3c4c8ab1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Original rows: 404\n",
            "Rows after filter: 404\n",
            "Num distinct labels: 18\n",
            "Example labels: ['cyanosis', 'dry scalp', 'edema', 'eye inflamation', 'eye redness', 'foot swelling', 'hand lump', 'itichy eyelid', 'knee swelling', 'lip swelling', 'mouth ulcer', 'neck swelling']\n",
            "Multi-label matrix shape: (404, 18)\n",
            "Saved /content/y_true.npy\n",
            "TF-IDF dim: 2000\n",
            "Loading wav2vec2 (this may download ~350MB)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "000b7ed6b18b4cd5965a3f4bfa3f0819"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c80474081bc74696be1ed139c31d1001"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "551a043580cd4ecf88cdf99549aac48b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a124ec4bb52846909ba8036a7d388aaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f8757f77547427fa8e96d79061f40ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c10f4b51f4b4376ad2c07ace615da9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2vec emb dim: 768\n",
            "Precomputing wav2vec embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404/404 [00:12<00:00, 33.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wav2vec embeddings saved to /content/wav2vec_tri_embs\n",
            "\n",
            "=== FOLD 1/5 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "324315a4d7c7459d923027a4df3a14d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 1: Loss 1.1866 | micro-F1 = 0.4106 | macro-F1 = 0.3213\n",
            "  -> saved best model for fold 1 (micro 0.4106)\n",
            "Fold 1 Epoch 2: Loss 1.0093 | micro-F1 = 0.4986 | macro-F1 = 0.4350\n",
            "  -> saved best model for fold 1 (micro 0.4986)\n",
            "Fold 1 Epoch 3: Loss 0.5365 | micro-F1 = 0.6049 | macro-F1 = 0.5135\n",
            "  -> saved best model for fold 1 (micro 0.6049)\n",
            "Fold 1 Epoch 4: Loss 0.2810 | micro-F1 = 0.7480 | macro-F1 = 0.6681\n",
            "  -> saved best model for fold 1 (micro 0.7480)\n",
            "Fold 1 Epoch 5: Loss 0.1795 | micro-F1 = 0.7229 | macro-F1 = 0.6587\n",
            "Fold 1 Epoch 6: Loss 0.1020 | micro-F1 = 0.7195 | macro-F1 = 0.6128\n",
            "Fold 1 Epoch 7: Loss 0.0670 | micro-F1 = 0.7339 | macro-F1 = 0.6594\n",
            "Fold 1 Epoch 8: Loss 0.0636 | micro-F1 = 0.7521 | macro-F1 = 0.6764\n",
            "  -> saved best model for fold 1 (micro 0.7521)\n",
            "Fold 1 Epoch 9: Loss 0.0345 | micro-F1 = 0.7242 | macro-F1 = 0.6368\n",
            "Fold 1 Epoch 10: Loss 0.0197 | micro-F1 = 0.7715 | macro-F1 = 0.6722\n",
            "  -> saved best model for fold 1 (micro 0.7715)\n",
            "Fold 1 Epoch 11: Loss 0.0245 | micro-F1 = 0.7495 | macro-F1 = 0.6773\n",
            "Fold 1 Epoch 12: Loss 0.0116 | micro-F1 = 0.7261 | macro-F1 = 0.6326\n",
            "Fold 1 Epoch 13: Loss 0.0090 | micro-F1 = 0.7599 | macro-F1 = 0.6941\n",
            "Fold 1 Epoch 14: Loss 0.0050 | micro-F1 = 0.7724 | macro-F1 = 0.7145\n",
            "  -> saved best model for fold 1 (micro 0.7724)\n",
            "Fold 1 Epoch 15: Loss 0.0048 | micro-F1 = 0.7585 | macro-F1 = 0.6699\n",
            "Fold 1 Epoch 16: Loss 0.0024 | micro-F1 = 0.7811 | macro-F1 = 0.7081\n",
            "  -> saved best model for fold 1 (micro 0.7811)\n",
            "Fold 1 Epoch 17: Loss 0.0016 | micro-F1 = 0.7782 | macro-F1 = 0.7046\n",
            "Fold 1 Epoch 18: Loss 0.0011 | micro-F1 = 0.7702 | macro-F1 = 0.6957\n",
            "Fold 1 Epoch 19: Loss 0.0009 | micro-F1 = 0.7699 | macro-F1 = 0.7064\n",
            "Fold 1 Epoch 20: Loss 0.0016 | micro-F1 = 0.7797 | macro-F1 = 0.7252\n",
            "Fold 1 done. Best micro-F1 on val: 0.7811\n",
            "\n",
            "=== FOLD 2/5 ===\n",
            "Fold 2 Epoch 1: Loss 1.1637 | micro-F1 = 0.4877 | macro-F1 = 0.3078\n",
            "  -> saved best model for fold 2 (micro 0.4877)\n",
            "Fold 2 Epoch 2: Loss 0.9417 | micro-F1 = 0.5636 | macro-F1 = 0.4017\n",
            "  -> saved best model for fold 2 (micro 0.5636)\n",
            "Fold 2 Epoch 3: Loss 0.5572 | micro-F1 = 0.7119 | macro-F1 = 0.5967\n",
            "  -> saved best model for fold 2 (micro 0.7119)\n",
            "Fold 2 Epoch 4: Loss 0.2810 | micro-F1 = 0.7283 | macro-F1 = 0.6594\n",
            "  -> saved best model for fold 2 (micro 0.7283)\n",
            "Fold 2 Epoch 5: Loss 0.1549 | micro-F1 = 0.7850 | macro-F1 = 0.6911\n",
            "  -> saved best model for fold 2 (micro 0.7850)\n",
            "Fold 2 Epoch 6: Loss 0.0925 | micro-F1 = 0.7975 | macro-F1 = 0.6950\n",
            "  -> saved best model for fold 2 (micro 0.7975)\n",
            "Fold 2 Epoch 7: Loss 0.0879 | micro-F1 = 0.7782 | macro-F1 = 0.6673\n",
            "Fold 2 Epoch 8: Loss 0.0452 | micro-F1 = 0.7992 | macro-F1 = 0.6268\n",
            "  -> saved best model for fold 2 (micro 0.7992)\n",
            "Fold 2 Epoch 9: Loss 0.0329 | micro-F1 = 0.8050 | macro-F1 = 0.6880\n",
            "  -> saved best model for fold 2 (micro 0.8050)\n",
            "Fold 2 Epoch 10: Loss 0.0189 | micro-F1 = 0.7748 | macro-F1 = 0.6628\n",
            "Fold 2 Epoch 11: Loss 0.0131 | micro-F1 = 0.8051 | macro-F1 = 0.6831\n",
            "  -> saved best model for fold 2 (micro 0.8051)\n",
            "Fold 2 Epoch 12: Loss 0.0112 | micro-F1 = 0.8042 | macro-F1 = 0.6641\n",
            "Fold 2 Epoch 13: Loss 0.0046 | micro-F1 = 0.8302 | macro-F1 = 0.7355\n",
            "  -> saved best model for fold 2 (micro 0.8302)\n",
            "Fold 2 Epoch 14: Loss 0.0028 | micro-F1 = 0.8108 | macro-F1 = 0.7005\n",
            "Fold 2 Epoch 15: Loss 0.0030 | micro-F1 = 0.8000 | macro-F1 = 0.6968\n",
            "Fold 2 Epoch 16: Loss 0.0022 | micro-F1 = 0.8075 | macro-F1 = 0.6680\n",
            "Fold 2 Epoch 17: Loss 0.0027 | micro-F1 = 0.8182 | macro-F1 = 0.7130\n",
            "Fold 2 Epoch 18: Loss 0.0015 | micro-F1 = 0.8253 | macro-F1 = 0.7272\n",
            "Fold 2 Epoch 19: Loss 0.0021 | micro-F1 = 0.8267 | macro-F1 = 0.7423\n",
            "Fold 2 Epoch 20: Loss 0.0010 | micro-F1 = 0.8109 | macro-F1 = 0.6864\n",
            "Fold 2 done. Best micro-F1 on val: 0.8302\n",
            "\n",
            "=== FOLD 3/5 ===\n",
            "Fold 3 Epoch 1: Loss 1.1779 | micro-F1 = 0.4062 | macro-F1 = 0.2996\n",
            "  -> saved best model for fold 3 (micro 0.4062)\n",
            "Fold 3 Epoch 2: Loss 0.9433 | micro-F1 = 0.5324 | macro-F1 = 0.4599\n",
            "  -> saved best model for fold 3 (micro 0.5324)\n",
            "Fold 3 Epoch 3: Loss 0.4742 | micro-F1 = 0.6835 | macro-F1 = 0.6098\n",
            "  -> saved best model for fold 3 (micro 0.6835)\n",
            "Fold 3 Epoch 4: Loss 0.2631 | micro-F1 = 0.7287 | macro-F1 = 0.6460\n",
            "  -> saved best model for fold 3 (micro 0.7287)\n",
            "Fold 3 Epoch 5: Loss 0.1576 | micro-F1 = 0.7520 | macro-F1 = 0.6447\n",
            "  -> saved best model for fold 3 (micro 0.7520)\n",
            "Fold 3 Epoch 6: Loss 0.1159 | micro-F1 = 0.7208 | macro-F1 = 0.6145\n",
            "Fold 3 Epoch 7: Loss 0.0864 | micro-F1 = 0.7291 | macro-F1 = 0.6395\n",
            "Fold 3 Epoch 8: Loss 0.0860 | micro-F1 = 0.7465 | macro-F1 = 0.6453\n",
            "Fold 3 Epoch 9: Loss 0.0480 | micro-F1 = 0.7541 | macro-F1 = 0.6564\n",
            "  -> saved best model for fold 3 (micro 0.7541)\n",
            "Fold 3 Epoch 10: Loss 0.0331 | micro-F1 = 0.7465 | macro-F1 = 0.6550\n",
            "Fold 3 Epoch 11: Loss 0.0153 | micro-F1 = 0.7500 | macro-F1 = 0.5993\n",
            "Fold 3 Epoch 12: Loss 0.0089 | micro-F1 = 0.7625 | macro-F1 = 0.6511\n",
            "  -> saved best model for fold 3 (micro 0.7625)\n",
            "Fold 3 Epoch 13: Loss 0.0062 | micro-F1 = 0.7673 | macro-F1 = 0.6456\n",
            "  -> saved best model for fold 3 (micro 0.7673)\n",
            "Fold 3 Epoch 14: Loss 0.0084 | micro-F1 = 0.7683 | macro-F1 = 0.6274\n",
            "  -> saved best model for fold 3 (micro 0.7683)\n",
            "Fold 3 Epoch 15: Loss 0.0052 | micro-F1 = 0.7578 | macro-F1 = 0.6475\n",
            "Fold 3 Epoch 16: Loss 0.0068 | micro-F1 = 0.7764 | macro-F1 = 0.6426\n",
            "  -> saved best model for fold 3 (micro 0.7764)\n",
            "Fold 3 Epoch 17: Loss 0.0031 | micro-F1 = 0.7629 | macro-F1 = 0.6217\n",
            "Fold 3 Epoch 18: Loss 0.0090 | micro-F1 = 0.7573 | macro-F1 = 0.6438\n",
            "Fold 3 Epoch 19: Loss 0.0035 | micro-F1 = 0.7664 | macro-F1 = 0.6518\n",
            "Fold 3 Epoch 20: Loss 0.0303 | micro-F1 = 0.7485 | macro-F1 = 0.6183\n",
            "Fold 3 done. Best micro-F1 on val: 0.7764\n",
            "\n",
            "=== FOLD 4/5 ===\n",
            "Fold 4 Epoch 1: Loss 1.1939 | micro-F1 = 0.4738 | macro-F1 = 0.3211\n",
            "  -> saved best model for fold 4 (micro 0.4738)\n",
            "Fold 4 Epoch 2: Loss 0.9008 | micro-F1 = 0.5317 | macro-F1 = 0.4308\n",
            "  -> saved best model for fold 4 (micro 0.5317)\n",
            "Fold 4 Epoch 3: Loss 0.5112 | micro-F1 = 0.6847 | macro-F1 = 0.5301\n",
            "  -> saved best model for fold 4 (micro 0.6847)\n",
            "Fold 4 Epoch 4: Loss 0.2577 | micro-F1 = 0.7332 | macro-F1 = 0.5872\n",
            "  -> saved best model for fold 4 (micro 0.7332)\n",
            "Fold 4 Epoch 5: Loss 0.1582 | micro-F1 = 0.7409 | macro-F1 = 0.6066\n",
            "  -> saved best model for fold 4 (micro 0.7409)\n",
            "Fold 4 Epoch 6: Loss 0.1315 | micro-F1 = 0.7727 | macro-F1 = 0.6568\n",
            "  -> saved best model for fold 4 (micro 0.7727)\n",
            "Fold 4 Epoch 7: Loss 0.0685 | micro-F1 = 0.7373 | macro-F1 = 0.5805\n",
            "Fold 4 Epoch 8: Loss 0.0563 | micro-F1 = 0.7655 | macro-F1 = 0.6360\n",
            "Fold 4 Epoch 9: Loss 0.0316 | micro-F1 = 0.7353 | macro-F1 = 0.5821\n",
            "Fold 4 Epoch 10: Loss 0.0439 | micro-F1 = 0.7474 | macro-F1 = 0.6241\n",
            "Fold 4 Epoch 11: Loss 0.0184 | micro-F1 = 0.7593 | macro-F1 = 0.6247\n",
            "Fold 4 Epoch 12: Loss 0.0119 | micro-F1 = 0.7552 | macro-F1 = 0.6606\n",
            "Fold 4 Epoch 13: Loss 0.0111 | micro-F1 = 0.7752 | macro-F1 = 0.6640\n",
            "  -> saved best model for fold 4 (micro 0.7752)\n",
            "Fold 4 Epoch 14: Loss 0.0057 | micro-F1 = 0.7643 | macro-F1 = 0.6216\n",
            "Fold 4 Epoch 15: Loss 0.0066 | micro-F1 = 0.7741 | macro-F1 = 0.6430\n",
            "Fold 4 Epoch 16: Loss 0.0053 | micro-F1 = 0.7490 | macro-F1 = 0.6072\n",
            "Fold 4 Epoch 17: Loss 0.0218 | micro-F1 = 0.7730 | macro-F1 = 0.6643\n",
            "Fold 4 Epoch 18: Loss 0.0032 | micro-F1 = 0.7568 | macro-F1 = 0.6548\n",
            "Fold 4 Epoch 19: Loss 0.0033 | micro-F1 = 0.7443 | macro-F1 = 0.6125\n",
            "Fold 4 Epoch 20: Loss 0.0015 | micro-F1 = 0.7377 | macro-F1 = 0.5837\n",
            "Fold 4 done. Best micro-F1 on val: 0.7752\n",
            "\n",
            "=== FOLD 5/5 ===\n",
            "Fold 5 Epoch 1: Loss 1.2048 | micro-F1 = 0.4268 | macro-F1 = 0.3184\n",
            "  -> saved best model for fold 5 (micro 0.4268)\n",
            "Fold 5 Epoch 2: Loss 1.0309 | micro-F1 = 0.5072 | macro-F1 = 0.3326\n",
            "  -> saved best model for fold 5 (micro 0.5072)\n",
            "Fold 5 Epoch 3: Loss 0.6536 | micro-F1 = 0.6168 | macro-F1 = 0.5169\n",
            "  -> saved best model for fold 5 (micro 0.6168)\n",
            "Fold 5 Epoch 4: Loss 0.3023 | micro-F1 = 0.7302 | macro-F1 = 0.6532\n",
            "  -> saved best model for fold 5 (micro 0.7302)\n",
            "Fold 5 Epoch 5: Loss 0.1662 | micro-F1 = 0.7490 | macro-F1 = 0.6466\n",
            "  -> saved best model for fold 5 (micro 0.7490)\n",
            "Fold 5 Epoch 6: Loss 0.1161 | micro-F1 = 0.7459 | macro-F1 = 0.6486\n",
            "Fold 5 Epoch 7: Loss 0.0768 | micro-F1 = 0.7206 | macro-F1 = 0.6080\n",
            "Fold 5 Epoch 8: Loss 0.0632 | micro-F1 = 0.7158 | macro-F1 = 0.5953\n",
            "Fold 5 Epoch 9: Loss 0.0331 | micro-F1 = 0.7401 | macro-F1 = 0.6293\n",
            "Fold 5 Epoch 10: Loss 0.0311 | micro-F1 = 0.7137 | macro-F1 = 0.6110\n",
            "Fold 5 Epoch 11: Loss 0.0196 | micro-F1 = 0.7314 | macro-F1 = 0.6397\n",
            "Fold 5 Epoch 12: Loss 0.0082 | micro-F1 = 0.7292 | macro-F1 = 0.6376\n",
            "Fold 5 Epoch 13: Loss 0.0053 | micro-F1 = 0.7333 | macro-F1 = 0.6267\n",
            "Fold 5 Epoch 14: Loss 0.0036 | micro-F1 = 0.7329 | macro-F1 = 0.6459\n",
            "Fold 5 Epoch 15: Loss 0.0035 | micro-F1 = 0.7322 | macro-F1 = 0.6333\n",
            "Fold 5 Epoch 16: Loss 0.0025 | micro-F1 = 0.7366 | macro-F1 = 0.6384\n",
            "Fold 5 Epoch 17: Loss 0.0020 | micro-F1 = 0.7227 | macro-F1 = 0.6124\n",
            "Fold 5 Epoch 18: Loss 0.0010 | micro-F1 = 0.7215 | macro-F1 = 0.6113\n",
            "Fold 5 Epoch 19: Loss 0.0015 | micro-F1 = 0.7441 | macro-F1 = 0.6398\n",
            "Fold 5 Epoch 20: Loss 0.0009 | micro-F1 = 0.7421 | macro-F1 = 0.6451\n",
            "Fold 5 done. Best micro-F1 on val: 0.7490\n",
            "Saved: /content/trimodal_oof_logits.npy, /content/trimodal_oof_probs.npy, /content/fold_ids.npy\n",
            "\n",
            "Best global threshold: 0.50 | micro-F1: 0.7822\n",
            "Final OOF micro-F1: 0.7822 | macro-F1: 0.6786\n",
            "All done. Files saved in /content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Text-Only Model (TF-IDF → MLP) with Final F1 Summary\n",
        "# Produces: text_only_oof_logits.npy and prints best threshold + micro/macro F1\n",
        "\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "import os, numpy as np, pandas as pd, gc, torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "SEED = 42\n",
        "CSV_PATH = \"/content/clips_with_audio_clean2.csv\"\n",
        "TEXT_COL = \"Updated_Question\"\n",
        "LABEL_COL = \"probable_classes\"\n",
        "FOLDS = 5\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df = df[df[TEXT_COL].notna() & df[LABEL_COL].notna()].reset_index(drop=True)\n",
        "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "# Label binarization\n",
        "raw_labels = df[LABEL_COL].astype(str).tolist()\n",
        "all_labels = sorted({tok.strip() for row in raw_labels for tok in row.split(',') if tok.strip()})\n",
        "label_to_idx = {lab: i for i, lab in enumerate(all_labels)}\n",
        "Y = np.zeros((len(df), len(all_labels)), dtype=np.float32)\n",
        "for i, row in enumerate(raw_labels):\n",
        "    for lab in row.split(','):\n",
        "        lab = lab.strip()\n",
        "        if lab in label_to_idx:\n",
        "            Y[i, label_to_idx[lab]] = 1.0\n",
        "num_labels = len(all_labels)\n",
        "\n",
        "# TF-IDF features\n",
        "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1,2), min_df=2)\n",
        "X_text_sparse = tfidf.fit_transform(texts).astype(np.float32)\n",
        "X = X_text_sparse.toarray()\n",
        "input_dim = X.shape[1]\n",
        "\n",
        "# Dataset\n",
        "class TextOnlyDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.Y[idx]\n",
        "\n",
        "# Model\n",
        "class TextMLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Linear(512, out_dim)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "# F1 calculation\n",
        "def f1_micro_macro(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n",
        "\n",
        "# Training\n",
        "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "oof_logits = np.zeros((len(X), num_labels), dtype=np.float32)\n",
        "\n",
        "for fold, (tr_idx, vl_idx) in enumerate(kf.split(X), start=1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    tr_ds = TextOnlyDataset(X[tr_idx], Y[tr_idx])\n",
        "    vl_ds = TextOnlyDataset(X[vl_idx], Y[vl_idx])\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    vl_loader = DataLoader(vl_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = TextMLP(input_dim, num_labels).to(device)\n",
        "    pos = Y[tr_idx].sum(axis=0) + 1e-6\n",
        "    neg = len(tr_idx) - pos + 1e-6\n",
        "    pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "    best_micro = 0.0\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        for xb, yb in tr_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Eval\n",
        "        model.eval()\n",
        "        all_logits, all_y = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in vl_loader:\n",
        "                out = model(xb.to(device))\n",
        "                all_logits.append(out.cpu())\n",
        "                all_y.append(yb)\n",
        "        logits = torch.cat(all_logits).numpy()\n",
        "        y_true = torch.cat(all_y).numpy()\n",
        "        prob = 1 / (1 + np.exp(-logits))\n",
        "        micro, _ = f1_micro_macro(y_true, prob)\n",
        "        print(f\"Fold {fold} Epoch {epoch}: micro-F1 = {micro:.4f}\")\n",
        "        if micro > best_micro:\n",
        "            best_micro = micro\n",
        "            best_logits = logits.copy()\n",
        "\n",
        "    oof_logits[vl_idx] = best_logits\n",
        "    print(f\"Fold {fold} done. Best micro-F1: {best_micro:.4f}\")\n",
        "    del model; torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# Final threshold search\n",
        "best_thr, best_micro, best_macro = 0.5, 0.0, 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 11):\n",
        "    micro, macro = f1_micro_macro(Y, 1 / (1 + np.exp(-oof_logits)), thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro, best_macro, best_thr = micro, macro, thr\n",
        "\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {best_micro:.4f} | macro-F1: {best_macro:.4f}\")\n",
        "\n",
        "np.save(\"/content/text_only_oof_logits.npy\", oof_logits)\n",
        "print(\"Saved: /content/text_only_oof_logits.npy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJgR6-KZut0M",
        "outputId": "b08dff2e-b69a-4ce7-9eb1-6538f8058ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 1 ===\n",
            "Fold 1 Epoch 1: micro-F1 = 0.5412\n",
            "Fold 1 Epoch 2: micro-F1 = 0.6462\n",
            "Fold 1 Epoch 3: micro-F1 = 0.7004\n",
            "Fold 1 Epoch 4: micro-F1 = 0.6976\n",
            "Fold 1 Epoch 5: micro-F1 = 0.6992\n",
            "Fold 1 Epoch 6: micro-F1 = 0.7094\n",
            "Fold 1 Epoch 7: micro-F1 = 0.7124\n",
            "Fold 1 Epoch 8: micro-F1 = 0.7048\n",
            "Fold 1 Epoch 9: micro-F1 = 0.7152\n",
            "Fold 1 Epoch 10: micro-F1 = 0.7248\n",
            "Fold 1 Epoch 11: micro-F1 = 0.7314\n",
            "Fold 1 Epoch 12: micro-F1 = 0.7211\n",
            "Fold 1 Epoch 13: micro-F1 = 0.7285\n",
            "Fold 1 Epoch 14: micro-F1 = 0.7356\n",
            "Fold 1 Epoch 15: micro-F1 = 0.7302\n",
            "Fold 1 Epoch 16: micro-F1 = 0.7302\n",
            "Fold 1 Epoch 17: micro-F1 = 0.7264\n",
            "Fold 1 Epoch 18: micro-F1 = 0.7289\n",
            "Fold 1 Epoch 19: micro-F1 = 0.7248\n",
            "Fold 1 Epoch 20: micro-F1 = 0.7206\n",
            "Fold 1 done. Best micro-F1: 0.7356\n",
            "\n",
            "=== Fold 2 ===\n",
            "Fold 2 Epoch 1: micro-F1 = 0.4693\n",
            "Fold 2 Epoch 2: micro-F1 = 0.6048\n",
            "Fold 2 Epoch 3: micro-F1 = 0.6772\n",
            "Fold 2 Epoch 4: micro-F1 = 0.7125\n",
            "Fold 2 Epoch 5: micro-F1 = 0.7082\n",
            "Fold 2 Epoch 6: micro-F1 = 0.7087\n",
            "Fold 2 Epoch 7: micro-F1 = 0.6941\n",
            "Fold 2 Epoch 8: micro-F1 = 0.6976\n",
            "Fold 2 Epoch 9: micro-F1 = 0.6976\n",
            "Fold 2 Epoch 10: micro-F1 = 0.6844\n",
            "Fold 2 Epoch 11: micro-F1 = 0.6846\n",
            "Fold 2 Epoch 12: micro-F1 = 0.6815\n",
            "Fold 2 Epoch 13: micro-F1 = 0.6711\n",
            "Fold 2 Epoch 14: micro-F1 = 0.6816\n",
            "Fold 2 Epoch 15: micro-F1 = 0.6787\n",
            "Fold 2 Epoch 16: micro-F1 = 0.6787\n",
            "Fold 2 Epoch 17: micro-F1 = 0.6862\n",
            "Fold 2 Epoch 18: micro-F1 = 0.6772\n",
            "Fold 2 Epoch 19: micro-F1 = 0.6817\n",
            "Fold 2 Epoch 20: micro-F1 = 0.6833\n",
            "Fold 2 done. Best micro-F1: 0.7125\n",
            "\n",
            "=== Fold 3 ===\n",
            "Fold 3 Epoch 1: micro-F1 = 0.4475\n",
            "Fold 3 Epoch 2: micro-F1 = 0.6011\n",
            "Fold 3 Epoch 3: micro-F1 = 0.6445\n",
            "Fold 3 Epoch 4: micro-F1 = 0.6680\n",
            "Fold 3 Epoch 5: micro-F1 = 0.6694\n",
            "Fold 3 Epoch 6: micro-F1 = 0.6709\n",
            "Fold 3 Epoch 7: micro-F1 = 0.6652\n",
            "Fold 3 Epoch 8: micro-F1 = 0.6667\n",
            "Fold 3 Epoch 9: micro-F1 = 0.6638\n",
            "Fold 3 Epoch 10: micro-F1 = 0.6681\n",
            "Fold 3 Epoch 11: micro-F1 = 0.6622\n",
            "Fold 3 Epoch 12: micro-F1 = 0.6696\n",
            "Fold 3 Epoch 13: micro-F1 = 0.6637\n",
            "Fold 3 Epoch 14: micro-F1 = 0.6622\n",
            "Fold 3 Epoch 15: micro-F1 = 0.6622\n",
            "Fold 3 Epoch 16: micro-F1 = 0.6562\n",
            "Fold 3 Epoch 17: micro-F1 = 0.6562\n",
            "Fold 3 Epoch 18: micro-F1 = 0.6622\n",
            "Fold 3 Epoch 19: micro-F1 = 0.6562\n",
            "Fold 3 Epoch 20: micro-F1 = 0.6667\n",
            "Fold 3 done. Best micro-F1: 0.6709\n",
            "\n",
            "=== Fold 4 ===\n",
            "Fold 4 Epoch 1: micro-F1 = 0.5102\n",
            "Fold 4 Epoch 2: micro-F1 = 0.6629\n",
            "Fold 4 Epoch 3: micro-F1 = 0.6944\n",
            "Fold 4 Epoch 4: micro-F1 = 0.7066\n",
            "Fold 4 Epoch 5: micro-F1 = 0.7104\n",
            "Fold 4 Epoch 6: micro-F1 = 0.7143\n",
            "Fold 4 Epoch 7: micro-F1 = 0.7209\n",
            "Fold 4 Epoch 8: micro-F1 = 0.7108\n",
            "Fold 4 Epoch 9: micro-F1 = 0.7130\n",
            "Fold 4 Epoch 10: micro-F1 = 0.7146\n",
            "Fold 4 Epoch 11: micro-F1 = 0.7130\n",
            "Fold 4 Epoch 12: micro-F1 = 0.7169\n",
            "Fold 4 Epoch 13: micro-F1 = 0.7156\n",
            "Fold 4 Epoch 14: micro-F1 = 0.7156\n",
            "Fold 4 Epoch 15: micro-F1 = 0.7126\n",
            "Fold 4 Epoch 16: micro-F1 = 0.7110\n",
            "Fold 4 Epoch 17: micro-F1 = 0.7110\n",
            "Fold 4 Epoch 18: micro-F1 = 0.7080\n",
            "Fold 4 Epoch 19: micro-F1 = 0.7140\n",
            "Fold 4 Epoch 20: micro-F1 = 0.7067\n",
            "Fold 4 done. Best micro-F1: 0.7209\n",
            "\n",
            "=== Fold 5 ===\n",
            "Fold 5 Epoch 1: micro-F1 = 0.4150\n",
            "Fold 5 Epoch 2: micro-F1 = 0.6214\n",
            "Fold 5 Epoch 3: micro-F1 = 0.6559\n",
            "Fold 5 Epoch 4: micro-F1 = 0.6763\n",
            "Fold 5 Epoch 5: micro-F1 = 0.6781\n",
            "Fold 5 Epoch 6: micro-F1 = 0.6911\n",
            "Fold 5 Epoch 7: micro-F1 = 0.6972\n",
            "Fold 5 Epoch 8: micro-F1 = 0.6987\n",
            "Fold 5 Epoch 9: micro-F1 = 0.6916\n",
            "Fold 5 Epoch 10: micro-F1 = 0.6933\n",
            "Fold 5 Epoch 11: micro-F1 = 0.6906\n",
            "Fold 5 Epoch 12: micro-F1 = 0.6964\n",
            "Fold 5 Epoch 13: micro-F1 = 0.6968\n",
            "Fold 5 Epoch 14: micro-F1 = 0.6955\n",
            "Fold 5 Epoch 15: micro-F1 = 0.6998\n",
            "Fold 5 Epoch 16: micro-F1 = 0.7029\n",
            "Fold 5 Epoch 17: micro-F1 = 0.7016\n",
            "Fold 5 Epoch 18: micro-F1 = 0.7032\n",
            "Fold 5 Epoch 19: micro-F1 = 0.6955\n",
            "Fold 5 Epoch 20: micro-F1 = 0.7002\n",
            "Fold 5 done. Best micro-F1: 0.7032\n",
            "\n",
            "Best global threshold: 0.50 | micro-F1: 0.7083 | macro-F1: 0.4934\n",
            "Saved: /content/text_only_oof_logits.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: Image-only baseline — EfficientNet-B3 (no resaving y_true/fold_ids)\n",
        "!pip install -q timm scikit-learn torchvision\n",
        "\n",
        "import os, gc, random, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "CSV_PATH = \"/content/clips_with_audio_clean2.csv\"\n",
        "IMG_COL = \"Image_path_final\"\n",
        "LABEL_COL = \"probable_classes\"\n",
        "FOLDS = 5\n",
        "SEED = 42\n",
        "IMG_SIZE = 300\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 12\n",
        "LR = 3e-4\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Load + Process ----------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "mask = df[IMG_COL].notna() & df[LABEL_COL].notna()\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Filtered rows:\", len(df))\n",
        "\n",
        "# load Y and fold_ids from tri-model run\n",
        "Y = np.load(\"/content/y_true.npy\")\n",
        "fold_ids = np.load(\"/content/fold_ids.npy\")\n",
        "n_labels = Y.shape[1]\n",
        "\n",
        "# multi-label argmax for primary class (used to verify stratification)\n",
        "primary = Y.argmax(axis=1)\n",
        "\n",
        "# ---------------- Transforms + Dataset ----------------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
        "])\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, df_slice, transform, Y):\n",
        "        self.df = df_slice.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.Y = Y\n",
        "        self.indices = df_slice.index.values\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        try:\n",
        "            img = Image.open(row[IMG_COL]).convert(\"RGB\")\n",
        "            img = self.transform(img)\n",
        "        except:\n",
        "            img = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
        "        y = torch.tensor(self.Y[self.indices[idx]])\n",
        "        return img, y\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "import timm\n",
        "class ImgOnlyNet(nn.Module):\n",
        "    def __init__(self, n_labels):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\"efficientnet_b3\", pretrained=True, num_classes=0)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(self.backbone.num_features, 512),\n",
        "            nn.ReLU(), nn.Dropout(0.4),\n",
        "            nn.Linear(512, n_labels)\n",
        "        )\n",
        "    def forward(self, x): return self.head(self.backbone(x))\n",
        "\n",
        "# ---------------- Metrics ----------------\n",
        "def f1_micro_macro(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n",
        "\n",
        "# ---------------- Train + Eval ----------------\n",
        "oof_logits = np.zeros_like(Y, dtype=np.float32)\n",
        "\n",
        "for fold in range(1, FOLDS+1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    vl_idx = np.where(fold_ids == fold)[0]\n",
        "    tr_idx = np.where(fold_ids != fold)[0]\n",
        "\n",
        "    model = ImgOnlyNet(n_labels).to(device)\n",
        "    tr_ds = ImgDataset(df.iloc[tr_idx], train_tf, Y)\n",
        "    vl_ds = ImgDataset(df.iloc[vl_idx], eval_tf, Y)\n",
        "    tr_loader = DataLoader(tr_ds, BATCH_SIZE, shuffle=True)\n",
        "    vl_loader = DataLoader(vl_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    y_tr = Y[tr_idx]\n",
        "    pos = y_tr.sum(axis=0) + 1e-6\n",
        "    neg = y_tr.shape[0] - pos + 1e-6\n",
        "    pos_weight = torch.tensor(neg / pos, device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_micro = 0.0\n",
        "    best_logits = None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        for x, y in tr_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x.to(device))\n",
        "            loss = criterion(logits, y.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_true = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in vl_loader:\n",
        "                logits = model(x.to(device))\n",
        "                all_logits.append(logits.cpu())\n",
        "                all_true.append(y)\n",
        "        logits_vl = torch.cat(all_logits).numpy()\n",
        "        y_true_vl = torch.cat(all_true).numpy()\n",
        "        probs = 1 / (1 + np.exp(-logits_vl))\n",
        "        micro, macro = f1_micro_macro(y_true_vl, probs)\n",
        "        print(f\"Fold {fold} Epoch {ep}: micro-F1={micro:.4f} | macro-F1={macro:.4f}\")\n",
        "        if micro > best_micro:\n",
        "            best_micro = micro\n",
        "            best_logits = logits_vl.copy()\n",
        "\n",
        "    oof_logits[vl_idx] = best_logits\n",
        "    print(f\"Fold {fold} done. Best val micro-F1: {best_micro:.4f}\")\n",
        "    del model; torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# ---------------- Save + Threshold Search ----------------\n",
        "np.save(\"/content/image_only_oof_logits.npy\", oof_logits)\n",
        "print(\"Saved /content/image_only_oof_logits.npy\")\n",
        "\n",
        "oof_probs = 1 / (1 + np.exp(-oof_logits))\n",
        "best_thr, best_micro, best_macro = 0.5, 0.0, 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 41):\n",
        "    micro, macro = f1_micro_macro(Y, oof_probs, thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro, best_macro, best_thr = micro, macro, thr\n",
        "\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {best_micro:.4f} | macro-F1: {best_macro:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtvim1CPxXWn",
        "outputId": "a51ff323-a987-47df-8c34-65dafcd60ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Filtered rows: 404\n",
            "\n",
            "=== Fold 1 ===\n",
            "Fold 1 Epoch 1: micro-F1=0.3754 | macro-F1=0.3475\n",
            "Fold 1 Epoch 2: micro-F1=0.4165 | macro-F1=0.3666\n",
            "Fold 1 Epoch 3: micro-F1=0.4353 | macro-F1=0.3689\n",
            "Fold 1 Epoch 4: micro-F1=0.4529 | macro-F1=0.3850\n",
            "Fold 1 Epoch 5: micro-F1=0.4709 | macro-F1=0.3966\n",
            "Fold 1 Epoch 6: micro-F1=0.4651 | macro-F1=0.3948\n",
            "Fold 1 Epoch 7: micro-F1=0.4812 | macro-F1=0.4030\n",
            "Fold 1 Epoch 8: micro-F1=0.4742 | macro-F1=0.4029\n",
            "Fold 1 Epoch 9: micro-F1=0.4771 | macro-F1=0.4038\n",
            "Fold 1 Epoch 10: micro-F1=0.4863 | macro-F1=0.4094\n",
            "Fold 1 Epoch 11: micro-F1=0.4801 | macro-F1=0.4062\n",
            "Fold 1 Epoch 12: micro-F1=0.4836 | macro-F1=0.3971\n",
            "Fold 1 done. Best val micro-F1: 0.4863\n",
            "\n",
            "=== Fold 2 ===\n",
            "Fold 2 Epoch 1: micro-F1=0.4168 | macro-F1=0.3731\n",
            "Fold 2 Epoch 2: micro-F1=0.4500 | macro-F1=0.3892\n",
            "Fold 2 Epoch 3: micro-F1=0.5367 | macro-F1=0.4472\n",
            "Fold 2 Epoch 4: micro-F1=0.5836 | macro-F1=0.4837\n",
            "Fold 2 Epoch 5: micro-F1=0.5789 | macro-F1=0.4812\n",
            "Fold 2 Epoch 6: micro-F1=0.5863 | macro-F1=0.4904\n",
            "Fold 2 Epoch 7: micro-F1=0.5799 | macro-F1=0.4790\n",
            "Fold 2 Epoch 8: micro-F1=0.5768 | macro-F1=0.4806\n",
            "Fold 2 Epoch 9: micro-F1=0.5862 | macro-F1=0.4835\n",
            "Fold 2 Epoch 10: micro-F1=0.5818 | macro-F1=0.4825\n",
            "Fold 2 Epoch 11: micro-F1=0.5791 | macro-F1=0.4842\n",
            "Fold 2 Epoch 12: micro-F1=0.5755 | macro-F1=0.4839\n",
            "Fold 2 done. Best val micro-F1: 0.5863\n",
            "\n",
            "=== Fold 3 ===\n",
            "Fold 3 Epoch 1: micro-F1=0.3628 | macro-F1=0.3306\n",
            "Fold 3 Epoch 2: micro-F1=0.4553 | macro-F1=0.3923\n",
            "Fold 3 Epoch 3: micro-F1=0.4871 | macro-F1=0.4137\n",
            "Fold 3 Epoch 4: micro-F1=0.5085 | macro-F1=0.4363\n",
            "Fold 3 Epoch 5: micro-F1=0.5429 | macro-F1=0.4613\n",
            "Fold 3 Epoch 6: micro-F1=0.5421 | macro-F1=0.4490\n",
            "Fold 3 Epoch 7: micro-F1=0.5690 | macro-F1=0.4759\n",
            "Fold 3 Epoch 8: micro-F1=0.5608 | macro-F1=0.4705\n",
            "Fold 3 Epoch 9: micro-F1=0.5686 | macro-F1=0.4733\n",
            "Fold 3 Epoch 10: micro-F1=0.5822 | macro-F1=0.4838\n",
            "Fold 3 Epoch 11: micro-F1=0.5731 | macro-F1=0.4788\n",
            "Fold 3 Epoch 12: micro-F1=0.5833 | macro-F1=0.4874\n",
            "Fold 3 done. Best val micro-F1: 0.5833\n",
            "\n",
            "=== Fold 4 ===\n",
            "Fold 4 Epoch 1: micro-F1=0.4482 | macro-F1=0.3980\n",
            "Fold 4 Epoch 2: micro-F1=0.5527 | macro-F1=0.4508\n",
            "Fold 4 Epoch 3: micro-F1=0.5133 | macro-F1=0.4374\n",
            "Fold 4 Epoch 4: micro-F1=0.5498 | macro-F1=0.4659\n",
            "Fold 4 Epoch 5: micro-F1=0.5599 | macro-F1=0.4633\n",
            "Fold 4 Epoch 6: micro-F1=0.5455 | macro-F1=0.4710\n",
            "Fold 4 Epoch 7: micro-F1=0.5862 | macro-F1=0.4817\n",
            "Fold 4 Epoch 8: micro-F1=0.5853 | macro-F1=0.4885\n",
            "Fold 4 Epoch 9: micro-F1=0.5685 | macro-F1=0.4750\n",
            "Fold 4 Epoch 10: micro-F1=0.5946 | macro-F1=0.4882\n",
            "Fold 4 Epoch 11: micro-F1=0.5854 | macro-F1=0.4945\n",
            "Fold 4 Epoch 12: micro-F1=0.5954 | macro-F1=0.4991\n",
            "Fold 4 done. Best val micro-F1: 0.5954\n",
            "\n",
            "=== Fold 5 ===\n",
            "Fold 5 Epoch 1: micro-F1=0.4148 | macro-F1=0.3769\n",
            "Fold 5 Epoch 2: micro-F1=0.4434 | macro-F1=0.3889\n",
            "Fold 5 Epoch 3: micro-F1=0.4785 | macro-F1=0.4171\n",
            "Fold 5 Epoch 4: micro-F1=0.5063 | macro-F1=0.4272\n",
            "Fold 5 Epoch 5: micro-F1=0.5209 | macro-F1=0.4403\n",
            "Fold 5 Epoch 6: micro-F1=0.5481 | macro-F1=0.4590\n",
            "Fold 5 Epoch 7: micro-F1=0.5492 | macro-F1=0.4689\n",
            "Fold 5 Epoch 8: micro-F1=0.5567 | macro-F1=0.4632\n",
            "Fold 5 Epoch 9: micro-F1=0.5476 | macro-F1=0.4660\n",
            "Fold 5 Epoch 10: micro-F1=0.5620 | macro-F1=0.4774\n",
            "Fold 5 Epoch 11: micro-F1=0.5650 | macro-F1=0.4761\n",
            "Fold 5 Epoch 12: micro-F1=0.5740 | macro-F1=0.4847\n",
            "Fold 5 done. Best val micro-F1: 0.5740\n",
            "Saved /content/image_only_oof_logits.npy\n",
            "\n",
            "Best global threshold: 0.79 | micro-F1: 0.6272 | macro-F1: 0.4980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: Audio-only baseline (wav2vec2 embeddings -> MLP), saves /content/audio_only_oof_logits.npy\n",
        "# Re-uses existing y_true.npy and fold_ids.npy if present; otherwise builds them and saves.\n",
        "\n",
        "!pip install -q transformers librosa soundfile scikit-learn tqdm\n",
        "\n",
        "import os, gc, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "CSV_PATH = \"/content/clips_with_audio_clean2.csv\"\n",
        "AUDIO_COL = \"audio_path\"\n",
        "LABEL_COL = \"probable_classes\"\n",
        "EMB_DIR = \"/content/wav2vec_tri_embs\"   # directory with emb_{row_id}.npy (if present)\n",
        "os.makedirs(EMB_DIR, exist_ok=True)\n",
        "\n",
        "FOLDS = 5\n",
        "SEED = 42\n",
        "TARGET_SR = 16000\n",
        "AUDIO_SEC = 3\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ------------------ LOAD CSV & BUILD LABELS ------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Original rows in CSV:\", len(df))\n",
        "\n",
        "# filter rows with valid audio path and labels\n",
        "mask = df[AUDIO_COL].notna() & df[LABEL_COL].notna()\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Rows after audio+label filter:\", len(df))\n",
        "\n",
        "# create multi-label matrix Y (N, C)\n",
        "raw_labels = df[LABEL_COL].astype(str).tolist()\n",
        "all_labels = sorted({tok.strip() for s in raw_labels for tok in s.split(\",\") if tok.strip()})\n",
        "label_to_idx = {lab:i for i,lab in enumerate(all_labels)}\n",
        "print(\"Num distinct labels:\", len(all_labels))\n",
        "N = len(df)\n",
        "\n",
        "Y = np.zeros((N, len(all_labels)), dtype=np.int8)\n",
        "for i, s in enumerate(raw_labels):\n",
        "    for tok in s.split(\",\"):\n",
        "        t = tok.strip()\n",
        "        if t in label_to_idx:\n",
        "            Y[i, label_to_idx[t]] = 1\n",
        "print(\"y shape:\", Y.shape)\n",
        "\n",
        "# save y_true if not present (useful for downstream CI code)\n",
        "if not os.path.exists(\"/content/y_true.npy\"):\n",
        "    np.save(\"/content/y_true.npy\", Y.astype(np.int8))\n",
        "    print(\"Saved /content/y_true.npy\")\n",
        "else:\n",
        "    print(\"/content/y_true.npy already exists; will reuse it.\")\n",
        "\n",
        "# ------------------ fold_ids (reproducible splits) ------------------\n",
        "if os.path.exists(\"/content/fold_ids.npy\"):\n",
        "    fold_ids = np.load(\"/content/fold_ids.npy\")\n",
        "    print(\"Loaded existing fold_ids.npy\")\n",
        "else:\n",
        "    print(\"Creating fold assignments (KFold).\")\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_ids = np.zeros(N, dtype=np.int32)\n",
        "    for fold, (_, vl_idx) in enumerate(kf.split(np.arange(N)), start=1):\n",
        "        fold_ids[vl_idx] = fold\n",
        "    np.save(\"/content/fold_ids.npy\", fold_ids)\n",
        "    print(\"Saved /content/fold_ids.npy\")\n",
        "\n",
        "# ------------------ wav2vec2 processor/model ------------------\n",
        "print(\"Loading wav2vec2 processor+model (may download ~350MB)...\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(DEVICE).eval()\n",
        "emb_dim = wav2vec.config.hidden_size\n",
        "print(\"wav2vec embed dim:\", emb_dim)\n",
        "\n",
        "# ------------------ Ensure audio embeddings exist (emb_{i}.npy) ------------------\n",
        "def build_audio_emb_if_missing(i, audio_path):\n",
        "    out_path = os.path.join(EMB_DIR, f\"emb_{i}.npy\")\n",
        "    if os.path.exists(out_path):\n",
        "        return\n",
        "    try:\n",
        "        y, sr = sf.read(audio_path, dtype=\"float32\")\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=None)  # fallback\n",
        "            y = y.astype(\"float32\")\n",
        "        except Exception as e2:\n",
        "            print(f\"[WARN] cannot read audio {audio_path}; saving zeros. Errors: {e1} / {e2}\")\n",
        "            np.save(out_path, np.zeros(emb_dim, dtype=np.float32))\n",
        "            return\n",
        "    # mono\n",
        "    if y.ndim > 1:\n",
        "        y = np.mean(y, axis=1)\n",
        "    # resample if needed\n",
        "    if sr != TARGET_SR:\n",
        "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
        "    needed = TARGET_SR * AUDIO_SEC\n",
        "    if len(y) < needed:\n",
        "        y = np.pad(y, (0, max(0, needed - len(y))))\n",
        "    else:\n",
        "        y = y[:needed]\n",
        "    with torch.no_grad():\n",
        "        inp = processor(y, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True)\n",
        "        out = wav2vec(inp.input_values.to(DEVICE))\n",
        "        emb = out.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy().astype(np.float32)\n",
        "    np.save(out_path, emb)\n",
        "\n",
        "print(\"Verifying/creating audio embeddings (this may take some minutes if missing)...\")\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    build_audio_emb_if_missing(i, row[AUDIO_COL])\n",
        "print(\"Audio embeddings ready under\", EMB_DIR)\n",
        "\n",
        "# ------------------ Load all audio embeddings into memory ------------------\n",
        "X_audio = np.zeros((N, emb_dim), dtype=np.float32)\n",
        "for i in range(N):\n",
        "    p = os.path.join(EMB_DIR, f\"emb_{i}.npy\")\n",
        "    if not os.path.exists(p):\n",
        "        X_audio[i] = np.zeros(emb_dim, dtype=np.float32)\n",
        "    else:\n",
        "        X_audio[i] = np.load(p).astype(np.float32)\n",
        "print(\"Loaded audio matrix shape:\", X_audio.shape)\n",
        "\n",
        "# ------------------ Simple MLP classifier ------------------\n",
        "class AudioNet(nn.Module):\n",
        "    def __init__(self, emb_dim, n_labels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, n_labels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def eval_f1(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n",
        "\n",
        "# ------------------ 5-Fold training using fold_ids (keep consistency) ------------------\n",
        "oof_logits = np.zeros_like(Y, dtype=np.float32)\n",
        "\n",
        "for fold in range(1, FOLDS+1):\n",
        "    print(f\"\\n=== FOLD {fold}/{FOLDS} ===\")\n",
        "    vl_idx = np.where(fold_ids == fold)[0]\n",
        "    tr_idx = np.where(fold_ids != fold)[0]\n",
        "\n",
        "    X_tr = torch.tensor(X_audio[tr_idx])\n",
        "    Y_tr = torch.tensor(Y[tr_idx]).float()\n",
        "    X_vl = torch.tensor(X_audio[vl_idx])\n",
        "    Y_vl = torch.tensor(Y[vl_idx]).float()\n",
        "\n",
        "    tr_ds = TensorDataset(X_tr, Y_tr)\n",
        "    vl_ds = TensorDataset(X_vl, Y_vl)\n",
        "\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "    vl_loader = DataLoader(vl_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = AudioNet(emb_dim, Y.shape[1]).to(DEVICE)\n",
        "\n",
        "    # pos_weight for imbalance\n",
        "    pos = Y_tr.numpy().sum(axis=0) + 1e-6\n",
        "    neg = Y_tr.shape[0] - pos + 1e-6\n",
        "    pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_micro = 0.0\n",
        "    best_logits_fold = None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in tr_loader:\n",
        "            xb = xb.to(DEVICE).float()\n",
        "            yb = yb.to(DEVICE).float()\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        # validation\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_labels_v = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in vl_loader:\n",
        "                xb = xb.to(DEVICE).float()\n",
        "                logits = model(xb).cpu().numpy()\n",
        "                all_logits.append(logits)\n",
        "                all_labels_v.append(yb.numpy())\n",
        "        all_logits = np.vstack(all_logits)\n",
        "        all_labels_v = np.vstack(all_labels_v)\n",
        "        probs = 1 / (1 + np.exp(-all_logits))\n",
        "        micro, macro = eval_f1(all_labels_v, probs, thr=0.5)\n",
        "        print(f\"Fold {fold} Epoch {ep}: loss={running_loss/len(tr_loader):.4f} micro-F1={micro:.4f} macro-F1={macro:.4f}\")\n",
        "        if micro > best_micro + 1e-6:\n",
        "            best_micro = micro\n",
        "            best_logits_fold = all_logits.copy()\n",
        "    if best_logits_fold is None:\n",
        "        # fallback: use last epoch logits\n",
        "        best_logits_fold = all_logits.copy()\n",
        "    oof_logits[vl_idx] = best_logits_fold\n",
        "    print(f\"Fold {fold} done. Best val micro-F1: {best_micro:.4f}\")\n",
        "    del model, optimizer, tr_loader, vl_loader\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# ------------------ Save OOF logits + report global metrics ------------------\n",
        "np.save(\"/content/audio_only_oof_logits.npy\", oof_logits)\n",
        "print(\"Saved /content/audio_only_oof_logits.npy\")\n",
        "\n",
        "oof_probs = 1 / (1 + np.exp(-oof_logits))\n",
        "best_thr = 0.5; best_micro = 0.0; best_macro = 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 51):\n",
        "    micro, macro = eval_f1(Y, oof_probs, thr=thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro, best_macro, best_thr = micro, macro, thr\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {best_micro:.4f} | macro-F1: {best_macro:.4f}\")\n",
        "print(\"Final OOF micro-F1:\", best_micro, \"macro-F1:\", best_macro)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8kT3JGW1t12",
        "outputId": "45d32d51-9bb1-4579-fec9-c45be66c994c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Original rows in CSV: 404\n",
            "Rows after audio+label filter: 404\n",
            "Num distinct labels: 18\n",
            "y shape: (404, 18)\n",
            "/content/y_true.npy already exists; will reuse it.\n",
            "Loaded existing fold_ids.npy\n",
            "Loading wav2vec2 processor+model (may download ~350MB)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2vec embed dim: 768\n",
            "Verifying/creating audio embeddings (this may take some minutes if missing)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404/404 [00:00<00:00, 20713.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio embeddings ready under /content/wav2vec_tri_embs\n",
            "Loaded audio matrix shape: (404, 768)\n",
            "\n",
            "=== FOLD 1/5 ===\n",
            "Fold 1 Epoch 1: loss=1.2478 micro-F1=0.2413 macro-F1=0.1375\n",
            "Fold 1 Epoch 2: loss=1.1833 micro-F1=0.3472 macro-F1=0.2233\n",
            "Fold 1 Epoch 3: loss=1.1790 micro-F1=0.4146 macro-F1=0.2021\n",
            "Fold 1 Epoch 4: loss=1.1940 micro-F1=0.2825 macro-F1=0.2119\n",
            "Fold 1 Epoch 5: loss=1.1760 micro-F1=0.3535 macro-F1=0.1480\n",
            "Fold 1 Epoch 6: loss=1.1592 micro-F1=0.3584 macro-F1=0.2149\n",
            "Fold 1 Epoch 7: loss=1.1570 micro-F1=0.3444 macro-F1=0.2078\n",
            "Fold 1 Epoch 8: loss=1.1593 micro-F1=0.3218 macro-F1=0.2212\n",
            "Fold 1 Epoch 9: loss=1.1271 micro-F1=0.2970 macro-F1=0.2156\n",
            "Fold 1 Epoch 10: loss=1.1378 micro-F1=0.4826 macro-F1=0.2877\n",
            "Fold 1 Epoch 11: loss=1.0553 micro-F1=0.3799 macro-F1=0.2796\n",
            "Fold 1 Epoch 12: loss=1.0421 micro-F1=0.4389 macro-F1=0.2914\n",
            "Fold 1 Epoch 13: loss=1.0254 micro-F1=0.3853 macro-F1=0.2942\n",
            "Fold 1 Epoch 14: loss=1.0461 micro-F1=0.3966 macro-F1=0.2463\n",
            "Fold 1 Epoch 15: loss=1.0413 micro-F1=0.3370 macro-F1=0.2753\n",
            "Fold 1 Epoch 16: loss=0.9962 micro-F1=0.4339 macro-F1=0.3135\n",
            "Fold 1 Epoch 17: loss=0.9530 micro-F1=0.3959 macro-F1=0.2801\n",
            "Fold 1 Epoch 18: loss=0.9860 micro-F1=0.3784 macro-F1=0.2918\n",
            "Fold 1 Epoch 19: loss=0.9623 micro-F1=0.4103 macro-F1=0.3170\n",
            "Fold 1 Epoch 20: loss=0.9720 micro-F1=0.4494 macro-F1=0.3146\n",
            "Fold 1 done. Best val micro-F1: 0.4826\n",
            "\n",
            "=== FOLD 2/5 ===\n",
            "Fold 2 Epoch 1: loss=1.2371 micro-F1=0.2845 macro-F1=0.1565\n",
            "Fold 2 Epoch 2: loss=1.1673 micro-F1=0.4524 macro-F1=0.1622\n",
            "Fold 2 Epoch 3: loss=1.1616 micro-F1=0.2106 macro-F1=0.1523\n",
            "Fold 2 Epoch 4: loss=1.1697 micro-F1=0.3065 macro-F1=0.2274\n",
            "Fold 2 Epoch 5: loss=1.1788 micro-F1=0.4268 macro-F1=0.2578\n",
            "Fold 2 Epoch 6: loss=1.1496 micro-F1=0.2765 macro-F1=0.1953\n",
            "Fold 2 Epoch 7: loss=1.1609 micro-F1=0.2211 macro-F1=0.1994\n",
            "Fold 2 Epoch 8: loss=1.1632 micro-F1=0.3657 macro-F1=0.2927\n",
            "Fold 2 Epoch 9: loss=1.1153 micro-F1=0.4396 macro-F1=0.2859\n",
            "Fold 2 Epoch 10: loss=1.1337 micro-F1=0.2118 macro-F1=0.1981\n",
            "Fold 2 Epoch 11: loss=1.0820 micro-F1=0.2814 macro-F1=0.2528\n",
            "Fold 2 Epoch 12: loss=1.0354 micro-F1=0.4213 macro-F1=0.2865\n",
            "Fold 2 Epoch 13: loss=1.0697 micro-F1=0.3663 macro-F1=0.2993\n",
            "Fold 2 Epoch 14: loss=1.0163 micro-F1=0.4259 macro-F1=0.2985\n",
            "Fold 2 Epoch 15: loss=0.9763 micro-F1=0.4613 macro-F1=0.3223\n",
            "Fold 2 Epoch 16: loss=0.9415 micro-F1=0.3929 macro-F1=0.3189\n",
            "Fold 2 Epoch 17: loss=0.9513 micro-F1=0.3195 macro-F1=0.2763\n",
            "Fold 2 Epoch 18: loss=0.9501 micro-F1=0.3662 macro-F1=0.2932\n",
            "Fold 2 Epoch 19: loss=0.9349 micro-F1=0.4992 macro-F1=0.3064\n",
            "Fold 2 Epoch 20: loss=0.9311 micro-F1=0.3961 macro-F1=0.3103\n",
            "Fold 2 done. Best val micro-F1: 0.4992\n",
            "\n",
            "=== FOLD 3/5 ===\n",
            "Fold 3 Epoch 1: loss=1.2327 micro-F1=0.3426 macro-F1=0.1969\n",
            "Fold 3 Epoch 2: loss=1.1819 micro-F1=0.3719 macro-F1=0.2311\n",
            "Fold 3 Epoch 3: loss=1.1700 micro-F1=0.3214 macro-F1=0.1982\n",
            "Fold 3 Epoch 4: loss=1.1779 micro-F1=0.3610 macro-F1=0.2336\n",
            "Fold 3 Epoch 5: loss=1.1805 micro-F1=0.4399 macro-F1=0.2102\n",
            "Fold 3 Epoch 6: loss=1.1479 micro-F1=0.3437 macro-F1=0.2442\n",
            "Fold 3 Epoch 7: loss=1.1657 micro-F1=0.2185 macro-F1=0.1788\n",
            "Fold 3 Epoch 8: loss=1.1456 micro-F1=0.4080 macro-F1=0.2606\n",
            "Fold 3 Epoch 9: loss=1.1202 micro-F1=0.3268 macro-F1=0.2449\n",
            "Fold 3 Epoch 10: loss=1.1350 micro-F1=0.3956 macro-F1=0.2383\n",
            "Fold 3 Epoch 11: loss=1.0955 micro-F1=0.3378 macro-F1=0.2720\n",
            "Fold 3 Epoch 12: loss=1.0456 micro-F1=0.3731 macro-F1=0.2960\n",
            "Fold 3 Epoch 13: loss=1.0330 micro-F1=0.3810 macro-F1=0.3073\n",
            "Fold 3 Epoch 14: loss=0.9929 micro-F1=0.4190 macro-F1=0.2959\n",
            "Fold 3 Epoch 15: loss=0.9381 micro-F1=0.4464 macro-F1=0.2696\n",
            "Fold 3 Epoch 16: loss=0.9800 micro-F1=0.4016 macro-F1=0.2860\n",
            "Fold 3 Epoch 17: loss=0.9307 micro-F1=0.4387 macro-F1=0.2960\n",
            "Fold 3 Epoch 18: loss=0.9741 micro-F1=0.4866 macro-F1=0.3001\n",
            "Fold 3 Epoch 19: loss=0.9335 micro-F1=0.3810 macro-F1=0.2878\n",
            "Fold 3 Epoch 20: loss=0.9340 micro-F1=0.3943 macro-F1=0.3122\n",
            "Fold 3 done. Best val micro-F1: 0.4866\n",
            "\n",
            "=== FOLD 4/5 ===\n",
            "Fold 4 Epoch 1: loss=1.2237 micro-F1=0.3228 macro-F1=0.1952\n",
            "Fold 4 Epoch 2: loss=1.1910 micro-F1=0.3055 macro-F1=0.1440\n",
            "Fold 4 Epoch 3: loss=1.1786 micro-F1=0.3625 macro-F1=0.2144\n",
            "Fold 4 Epoch 4: loss=1.1731 micro-F1=0.2883 macro-F1=0.2155\n",
            "Fold 4 Epoch 5: loss=1.1514 micro-F1=0.3895 macro-F1=0.2221\n",
            "Fold 4 Epoch 6: loss=1.1757 micro-F1=0.2902 macro-F1=0.1908\n",
            "Fold 4 Epoch 7: loss=1.1496 micro-F1=0.3801 macro-F1=0.2427\n",
            "Fold 4 Epoch 8: loss=1.1619 micro-F1=0.4494 macro-F1=0.2575\n",
            "Fold 4 Epoch 9: loss=1.1380 micro-F1=0.3432 macro-F1=0.2840\n",
            "Fold 4 Epoch 10: loss=1.1250 micro-F1=0.4168 macro-F1=0.3177\n",
            "Fold 4 Epoch 11: loss=1.0843 micro-F1=0.4640 macro-F1=0.3597\n",
            "Fold 4 Epoch 12: loss=1.0680 micro-F1=0.4293 macro-F1=0.3164\n",
            "Fold 4 Epoch 13: loss=1.0193 micro-F1=0.3881 macro-F1=0.3223\n",
            "Fold 4 Epoch 14: loss=1.0555 micro-F1=0.4202 macro-F1=0.3113\n",
            "Fold 4 Epoch 15: loss=0.9765 micro-F1=0.4453 macro-F1=0.3568\n",
            "Fold 4 Epoch 16: loss=0.9851 micro-F1=0.4417 macro-F1=0.3542\n",
            "Fold 4 Epoch 17: loss=0.9560 micro-F1=0.4654 macro-F1=0.3599\n",
            "Fold 4 Epoch 18: loss=0.9161 micro-F1=0.4053 macro-F1=0.3260\n",
            "Fold 4 Epoch 19: loss=0.8853 micro-F1=0.3672 macro-F1=0.3099\n",
            "Fold 4 Epoch 20: loss=0.8821 micro-F1=0.4188 macro-F1=0.3205\n",
            "Fold 4 done. Best val micro-F1: 0.4654\n",
            "\n",
            "=== FOLD 5/5 ===\n",
            "Fold 5 Epoch 1: loss=1.2471 micro-F1=0.3930 macro-F1=0.1663\n",
            "Fold 5 Epoch 2: loss=1.2069 micro-F1=0.3633 macro-F1=0.2764\n",
            "Fold 5 Epoch 3: loss=1.1900 micro-F1=0.2209 macro-F1=0.1557\n",
            "Fold 5 Epoch 4: loss=1.1920 micro-F1=0.3321 macro-F1=0.2177\n",
            "Fold 5 Epoch 5: loss=1.1393 micro-F1=0.3724 macro-F1=0.2022\n",
            "Fold 5 Epoch 6: loss=1.1577 micro-F1=0.4282 macro-F1=0.2865\n",
            "Fold 5 Epoch 7: loss=1.1537 micro-F1=0.3597 macro-F1=0.2144\n",
            "Fold 5 Epoch 8: loss=1.1212 micro-F1=0.3927 macro-F1=0.2638\n",
            "Fold 5 Epoch 9: loss=1.1288 micro-F1=0.3358 macro-F1=0.2457\n",
            "Fold 5 Epoch 10: loss=1.1346 micro-F1=0.4005 macro-F1=0.3061\n",
            "Fold 5 Epoch 11: loss=1.0919 micro-F1=0.3365 macro-F1=0.2784\n",
            "Fold 5 Epoch 12: loss=1.0841 micro-F1=0.3752 macro-F1=0.2542\n",
            "Fold 5 Epoch 13: loss=1.0541 micro-F1=0.3597 macro-F1=0.2964\n",
            "Fold 5 Epoch 14: loss=1.0583 micro-F1=0.3488 macro-F1=0.3193\n",
            "Fold 5 Epoch 15: loss=1.0145 micro-F1=0.4343 macro-F1=0.3521\n",
            "Fold 5 Epoch 16: loss=0.9767 micro-F1=0.4497 macro-F1=0.3381\n",
            "Fold 5 Epoch 17: loss=0.9417 micro-F1=0.4649 macro-F1=0.3214\n",
            "Fold 5 Epoch 18: loss=0.9488 micro-F1=0.4473 macro-F1=0.3428\n",
            "Fold 5 Epoch 19: loss=0.9155 micro-F1=0.4809 macro-F1=0.3511\n",
            "Fold 5 Epoch 20: loss=0.9106 micro-F1=0.4862 macro-F1=0.3439\n",
            "Fold 5 done. Best val micro-F1: 0.4862\n",
            "Saved /content/audio_only_oof_logits.npy\n",
            "\n",
            "Best global threshold: 0.58 | micro-F1: 0.5004 | macro-F1: 0.3230\n",
            "Final OOF micro-F1: 0.5003728560775541 macro-F1: 0.3229979795573962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: Text + Audio baseline (TF-IDF + wav2vec2 emb -> MLP fusion)\n",
        "# Saves /content/text_audio_oof_logits.npy and updates summary CSV.\n",
        "!pip install -q scikit-learn transformers librosa soundfile tqdm\n",
        "\n",
        "import os, gc, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "CSV_PATH = \"/content/clips_with_audio_clean2.csv\"\n",
        "AUDIO_COL = \"audio_path\"\n",
        "TEXT_COL_CAND = [\"Updated_Question\", \"Question_summ\"]\n",
        "LABEL_COL = \"probable_classes\"\n",
        "\n",
        "EMB_DIR = \"/content/wav2vec_tri_embs\"   # precomputed emb_{i}.npy\n",
        "os.makedirs(EMB_DIR, exist_ok=True)\n",
        "\n",
        "OUT_OOF = \"/content/text_audio_oof_logits.npy\"\n",
        "SUMMARY_CSV = \"/content/analysis_outputs/summary_metrics.csv\"\n",
        "os.makedirs(os.path.dirname(SUMMARY_CSV), exist_ok=True)\n",
        "\n",
        "FOLDS = 5\n",
        "SEED = 42\n",
        "TARGET_SR = 16000\n",
        "AUDIO_SEC = 3\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "TFIDF_DIM = 2000\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---------------- Load CSV and select text column ----------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Original rows:\", len(df))\n",
        "\n",
        "TEXT_COL = None\n",
        "for c in TEXT_COL_CAND:\n",
        "    if c in df.columns:\n",
        "        TEXT_COL = c\n",
        "        break\n",
        "if TEXT_COL is None:\n",
        "    raise ValueError(f\"No text column found in CSV among {TEXT_COL_CAND}\")\n",
        "\n",
        "# filter rows with audio/text/label\n",
        "mask = df[AUDIO_COL].notna() & df[TEXT_COL].notna() & df[LABEL_COL].notna()\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Rows after filter:\", len(df))\n",
        "\n",
        "# ---------------- Build multi-label matrix ----------------\n",
        "raw_labels = df[LABEL_COL].astype(str).tolist()\n",
        "all_labels = sorted({tok.strip() for s in raw_labels for tok in s.split(\",\") if tok.strip()})\n",
        "label_to_idx = {lab:i for i,lab in enumerate(all_labels)}\n",
        "print(\"Num labels:\", len(all_labels))\n",
        "N = len(df)\n",
        "\n",
        "Y = np.zeros((N, len(all_labels)), dtype=np.int8)\n",
        "for i, s in enumerate(raw_labels):\n",
        "    for tok in s.split(\",\"):\n",
        "        t = tok.strip()\n",
        "        if t in label_to_idx:\n",
        "            Y[i, label_to_idx[t]] = 1\n",
        "print(\"Y shape:\", Y.shape)\n",
        "\n",
        "# save y_true.npy if missing (useful later for CI code)\n",
        "if not os.path.exists(\"/content/y_true.npy\"):\n",
        "    np.save(\"/content/y_true.npy\", Y.astype(np.int8))\n",
        "    print(\"Saved /content/y_true.npy\")\n",
        "else:\n",
        "    print(\"/content/y_true.npy exists; reusing it.\")\n",
        "\n",
        "# ---------------- fold assignments ----------------\n",
        "if os.path.exists(\"/content/fold_ids.npy\"):\n",
        "    fold_ids = np.load(\"/content/fold_ids.npy\")\n",
        "    print(\"Loaded existing /content/fold_ids.npy\")\n",
        "else:\n",
        "    print(\"Creating KFold assignments and saving /content/fold_ids.npy\")\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_ids = np.zeros(N, dtype=np.int32)\n",
        "    for fold, (_, vl_idx) in enumerate(kf.split(np.arange(N)), start=1):\n",
        "        fold_ids[vl_idx] = fold\n",
        "    np.save(\"/content/fold_ids.npy\", fold_ids)\n",
        "    print(\"Saved /content/fold_ids.npy\")\n",
        "\n",
        "# ---------------- wav2vec2 (for fallback embedding creation) ----------------\n",
        "print(\"Loading wav2vec2 processor+model (may download ~350MB)...\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(DEVICE).eval()\n",
        "emb_dim = wav2vec.config.hidden_size\n",
        "print(\"wav2vec dim:\", emb_dim)\n",
        "\n",
        "# ---------------- Ensure audio embeddings exist ----------------\n",
        "def build_audio_emb_if_missing(i, audio_path):\n",
        "    out_path = os.path.join(EMB_DIR, f\"emb_{i}.npy\")\n",
        "    if os.path.exists(out_path):\n",
        "        return\n",
        "    try:\n",
        "        y, sr = sf.read(audio_path, dtype=\"float32\")\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=None)\n",
        "            y = y.astype(\"float32\")\n",
        "        except Exception as e2:\n",
        "            print(f\"[WARN] cannot read audio {audio_path}; saving zeros. Errors: {e1}/{e2}\")\n",
        "            np.save(out_path, np.zeros(emb_dim, dtype=np.float32))\n",
        "            return\n",
        "    if y.ndim > 1:\n",
        "        y = np.mean(y, axis=1)\n",
        "    if sr != TARGET_SR:\n",
        "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
        "    needed = TARGET_SR * AUDIO_SEC\n",
        "    if len(y) < needed:\n",
        "        y = np.pad(y, (0, max(0, needed - len(y))))\n",
        "    else:\n",
        "        y = y[:needed]\n",
        "    with torch.no_grad():\n",
        "        inp = processor(y, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True)\n",
        "        out = wav2vec(inp.input_values.to(DEVICE))\n",
        "        emb = out.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy().astype(np.float32)\n",
        "    np.save(out_path, emb)\n",
        "\n",
        "print(\"Verifying/creating audio embeddings (may take time if missing)...\")\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    build_audio_emb_if_missing(i, row[AUDIO_COL])\n",
        "print(\"Audio embeddings ready in\", EMB_DIR)\n",
        "\n",
        "# ---------------- Load audio embeddings matrix ----------------\n",
        "X_audio = np.zeros((N, emb_dim), dtype=np.float32)\n",
        "for i in range(N):\n",
        "    p = os.path.join(EMB_DIR, f\"emb_{i}.npy\")\n",
        "    if os.path.exists(p):\n",
        "        X_audio[i] = np.load(p).astype(np.float32)\n",
        "    else:\n",
        "        X_audio[i] = np.zeros(emb_dim, dtype=np.float32)\n",
        "print(\"Loaded X_audio shape:\", X_audio.shape)\n",
        "\n",
        "# ---------------- TF-IDF (fit on full texts) ----------------\n",
        "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
        "tfidf = TfidfVectorizer(max_features=TFIDF_DIM, ngram_range=(1,2), min_df=1)\n",
        "X_text_sparse = tfidf.fit_transform(texts)\n",
        "text_dim = X_text_sparse.shape[1]\n",
        "print(\"TF-IDF dim:\", text_dim)\n",
        "\n",
        "# dense text matrix (N, text_dim)\n",
        "X_text = X_text_sparse.toarray().astype(np.float32)\n",
        "\n",
        "# ---------------- Build fused features: concat text + audio ----------------\n",
        "X_fused = np.concatenate([X_text, X_audio], axis=1)\n",
        "fusion_dim = X_fused.shape[1]\n",
        "print(\"Fused feature dim:\", fusion_dim)\n",
        "\n",
        "# ---------------- Simple MLP classifier ----------------\n",
        "class FusedNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_labels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, n_labels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def eval_f1(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n",
        "\n",
        "# ---------------- 5-fold training using fold_ids ----------------\n",
        "oof_logits = np.zeros((N, len(all_labels)), dtype=np.float32)\n",
        "\n",
        "for fold in range(1, FOLDS+1):\n",
        "    print(f\"\\n=== FOLD {fold}/{FOLDS} ===\")\n",
        "    vl_idx = np.where(fold_ids == fold)[0]\n",
        "    tr_idx = np.where(fold_ids != fold)[0]\n",
        "\n",
        "    X_tr = torch.tensor(X_fused[tr_idx]).float()\n",
        "    Y_tr = torch.tensor(Y[tr_idx]).float()\n",
        "    X_vl = torch.tensor(X_fused[vl_idx]).float()\n",
        "    Y_vl = torch.tensor(Y[vl_idx]).float()\n",
        "\n",
        "    tr_ds = TensorDataset(X_tr, Y_tr)\n",
        "    vl_ds = TensorDataset(X_vl, Y_vl)\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    vl_loader = DataLoader(vl_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = FusedNet(fusion_dim, len(all_labels)).to(DEVICE)\n",
        "\n",
        "    # pos_weight from training labels\n",
        "    pos = Y_tr.numpy().sum(axis=0) + 1e-6\n",
        "    neg = Y_tr.shape[0] - pos + 1e-6\n",
        "    pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "    best_micro = 0.0\n",
        "    best_logits_fold = None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in tr_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_y = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in vl_loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                out = model(xb).cpu().numpy()\n",
        "                all_logits.append(out)\n",
        "                all_y.append(yb.numpy())\n",
        "        all_logits = np.vstack(all_logits)\n",
        "        all_y = np.vstack(all_y)\n",
        "        probs = 1 / (1 + np.exp(-all_logits))\n",
        "        micro, macro = eval_f1(all_y, probs, thr=0.5)\n",
        "        print(f\"Fold {fold} Epoch {ep}: loss={running_loss/len(tr_loader):.4f} micro-F1={micro:.4f} macro-F1={macro:.4f}\")\n",
        "        if micro > best_micro + 1e-6:\n",
        "            best_micro = micro\n",
        "            best_logits_fold = all_logits.copy()\n",
        "\n",
        "    if best_logits_fold is None:\n",
        "        best_logits_fold = all_logits.copy()  # fallback\n",
        "    oof_logits[vl_idx] = best_logits_fold\n",
        "    print(f\"Fold {fold} done. Best val micro-F1 (fold): {best_micro:.4f}\")\n",
        "\n",
        "    del model, optimizer, tr_loader, vl_loader\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# ---------------- global threshold search and save ----------------\n",
        "np.save(OUT_OOF, oof_logits)\n",
        "print(\"Saved OOF logits to\", OUT_OOF)\n",
        "\n",
        "oof_probs = 1 / (1 + np.exp(-oof_logits))\n",
        "best_thr = 0.5; best_micro = 0.0; best_macro = 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 51):\n",
        "    micro, macro = eval_f1(Y, oof_probs, thr=thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro, best_macro, best_thr = micro, macro, thr\n",
        "\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {best_micro:.4f} | macro-F1: {best_macro:.4f}\")\n",
        "\n",
        "# ---------------- update summary CSV ----------------\n",
        "row = {\"model\":\"text+audio\", \"micro_f1\":float(best_micro), \"macro_f1\":float(best_macro), \"threshold\":float(best_thr)}\n",
        "if os.path.exists(SUMMARY_CSV):\n",
        "    df_sum = pd.read_csv(SUMMARY_CSV)\n",
        "    # replace row if exists\n",
        "    df_sum = df_sum[df_sum.model != \"text+audio\"].append(row, ignore_index=True)\n",
        "else:\n",
        "    df_sum = pd.DataFrame([row])\n",
        "df_sum.to_csv(SUMMARY_CSV, index=False)\n",
        "print(\"Updated summary CSV at\", SUMMARY_CSV)\n",
        "\n",
        "print(\"Done (Text + Audio).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqizPZrL34f_",
        "outputId": "5da580c6-a9e6-4631-897e-e89f651f7205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Original rows: 404\n",
            "Rows after filter: 404\n",
            "Num labels: 18\n",
            "Y shape: (404, 18)\n",
            "/content/y_true.npy exists; reusing it.\n",
            "Loaded existing /content/fold_ids.npy\n",
            "Loading wav2vec2 processor+model (may download ~350MB)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2vec dim: 768\n",
            "Verifying/creating audio embeddings (may take time if missing)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404/404 [00:00<00:00, 14194.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio embeddings ready in /content/wav2vec_tri_embs\n",
            "Loaded X_audio shape: (404, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF dim: 2000\n",
            "Fused feature dim: 2768\n",
            "\n",
            "=== FOLD 1/5 ===\n",
            "Fold 1 Epoch 1: loss=1.2440 micro-F1=0.2940 macro-F1=0.1331\n",
            "Fold 1 Epoch 2: loss=1.2063 micro-F1=0.2372 macro-F1=0.1723\n",
            "Fold 1 Epoch 3: loss=1.1851 micro-F1=0.2528 macro-F1=0.1979\n",
            "Fold 1 Epoch 4: loss=1.1536 micro-F1=0.3931 macro-F1=0.2724\n",
            "Fold 1 Epoch 5: loss=1.0566 micro-F1=0.4338 macro-F1=0.2798\n",
            "Fold 1 Epoch 6: loss=0.8940 micro-F1=0.5742 macro-F1=0.4105\n",
            "Fold 1 Epoch 7: loss=0.6707 micro-F1=0.5989 macro-F1=0.4581\n",
            "Fold 1 Epoch 8: loss=0.5197 micro-F1=0.6579 macro-F1=0.4791\n",
            "Fold 1 Epoch 9: loss=0.3976 micro-F1=0.6378 macro-F1=0.5221\n",
            "Fold 1 Epoch 10: loss=0.3422 micro-F1=0.6616 macro-F1=0.5615\n",
            "Fold 1 Epoch 11: loss=0.2794 micro-F1=0.6918 macro-F1=0.5555\n",
            "Fold 1 Epoch 12: loss=0.2435 micro-F1=0.6826 macro-F1=0.5396\n",
            "Fold 1 Epoch 13: loss=0.2198 micro-F1=0.6627 macro-F1=0.4835\n",
            "Fold 1 Epoch 14: loss=0.1835 micro-F1=0.6786 macro-F1=0.5705\n",
            "Fold 1 Epoch 15: loss=0.1613 micro-F1=0.7020 macro-F1=0.5762\n",
            "Fold 1 Epoch 16: loss=0.1432 micro-F1=0.6802 macro-F1=0.5793\n",
            "Fold 1 Epoch 17: loss=0.1413 micro-F1=0.6543 macro-F1=0.4518\n",
            "Fold 1 Epoch 18: loss=0.1394 micro-F1=0.6949 macro-F1=0.5196\n",
            "Fold 1 Epoch 19: loss=0.1157 micro-F1=0.6488 macro-F1=0.4821\n",
            "Fold 1 Epoch 20: loss=0.1101 micro-F1=0.6817 macro-F1=0.5350\n",
            "Fold 1 done. Best val micro-F1 (fold): 0.7020\n",
            "\n",
            "=== FOLD 2/5 ===\n",
            "Fold 2 Epoch 1: loss=1.3315 micro-F1=0.1151 macro-F1=0.0703\n",
            "Fold 2 Epoch 2: loss=1.1695 micro-F1=0.3692 macro-F1=0.2080\n",
            "Fold 2 Epoch 3: loss=1.1373 micro-F1=0.2889 macro-F1=0.1577\n",
            "Fold 2 Epoch 4: loss=1.0958 micro-F1=0.4098 macro-F1=0.2987\n",
            "Fold 2 Epoch 5: loss=1.0079 micro-F1=0.4679 macro-F1=0.3488\n",
            "Fold 2 Epoch 6: loss=0.8435 micro-F1=0.6362 macro-F1=0.4449\n",
            "Fold 2 Epoch 7: loss=0.5799 micro-F1=0.6800 macro-F1=0.5178\n",
            "Fold 2 Epoch 8: loss=0.4449 micro-F1=0.7256 macro-F1=0.5905\n",
            "Fold 2 Epoch 9: loss=0.3549 micro-F1=0.6691 macro-F1=0.5351\n",
            "Fold 2 Epoch 10: loss=0.2971 micro-F1=0.6768 macro-F1=0.5321\n",
            "Fold 2 Epoch 11: loss=0.2517 micro-F1=0.6932 macro-F1=0.5405\n",
            "Fold 2 Epoch 12: loss=0.2233 micro-F1=0.6909 macro-F1=0.4471\n",
            "Fold 2 Epoch 13: loss=0.1998 micro-F1=0.7152 macro-F1=0.5301\n",
            "Fold 2 Epoch 14: loss=0.1854 micro-F1=0.7076 macro-F1=0.5186\n",
            "Fold 2 Epoch 15: loss=0.1649 micro-F1=0.7084 macro-F1=0.5719\n",
            "Fold 2 Epoch 16: loss=0.1449 micro-F1=0.6888 macro-F1=0.5276\n",
            "Fold 2 Epoch 17: loss=0.1297 micro-F1=0.7254 macro-F1=0.5443\n",
            "Fold 2 Epoch 18: loss=0.1244 micro-F1=0.6983 macro-F1=0.5472\n",
            "Fold 2 Epoch 19: loss=0.1133 micro-F1=0.6914 macro-F1=0.5638\n",
            "Fold 2 Epoch 20: loss=0.1042 micro-F1=0.7032 macro-F1=0.4800\n",
            "Fold 2 done. Best val micro-F1 (fold): 0.7256\n",
            "\n",
            "=== FOLD 3/5 ===\n",
            "Fold 3 Epoch 1: loss=1.3012 micro-F1=0.3091 macro-F1=0.1196\n",
            "Fold 3 Epoch 2: loss=1.2278 micro-F1=0.3277 macro-F1=0.2270\n",
            "Fold 3 Epoch 3: loss=1.2457 micro-F1=0.3562 macro-F1=0.1837\n",
            "Fold 3 Epoch 4: loss=1.1391 micro-F1=0.4467 macro-F1=0.2982\n",
            "Fold 3 Epoch 5: loss=1.0650 micro-F1=0.4528 macro-F1=0.3854\n",
            "Fold 3 Epoch 6: loss=0.8630 micro-F1=0.4958 macro-F1=0.4318\n",
            "Fold 3 Epoch 7: loss=0.6438 micro-F1=0.4906 macro-F1=0.4219\n",
            "Fold 3 Epoch 8: loss=0.5245 micro-F1=0.5923 macro-F1=0.4611\n",
            "Fold 3 Epoch 9: loss=0.3893 micro-F1=0.6180 macro-F1=0.4384\n",
            "Fold 3 Epoch 10: loss=0.3387 micro-F1=0.6667 macro-F1=0.5178\n",
            "Fold 3 Epoch 11: loss=0.2629 micro-F1=0.6627 macro-F1=0.5515\n",
            "Fold 3 Epoch 12: loss=0.2342 micro-F1=0.6719 macro-F1=0.4490\n",
            "Fold 3 Epoch 13: loss=0.1995 micro-F1=0.6706 macro-F1=0.4535\n",
            "Fold 3 Epoch 14: loss=0.1838 micro-F1=0.6883 macro-F1=0.5064\n",
            "Fold 3 Epoch 15: loss=0.1624 micro-F1=0.6876 macro-F1=0.5117\n",
            "Fold 3 Epoch 16: loss=0.1551 micro-F1=0.6892 macro-F1=0.5117\n",
            "Fold 3 Epoch 17: loss=0.1324 micro-F1=0.6775 macro-F1=0.4590\n",
            "Fold 3 Epoch 18: loss=0.1247 micro-F1=0.6640 macro-F1=0.4682\n",
            "Fold 3 Epoch 19: loss=0.1148 micro-F1=0.6871 macro-F1=0.4773\n",
            "Fold 3 Epoch 20: loss=0.1092 micro-F1=0.6612 macro-F1=0.4983\n",
            "Fold 3 done. Best val micro-F1 (fold): 0.6892\n",
            "\n",
            "=== FOLD 4/5 ===\n",
            "Fold 4 Epoch 1: loss=1.2778 micro-F1=0.2744 macro-F1=0.1779\n",
            "Fold 4 Epoch 2: loss=1.1892 micro-F1=0.4622 macro-F1=0.1455\n",
            "Fold 4 Epoch 3: loss=1.1488 micro-F1=0.2018 macro-F1=0.1510\n",
            "Fold 4 Epoch 4: loss=1.1427 micro-F1=0.4008 macro-F1=0.2593\n",
            "Fold 4 Epoch 5: loss=1.0645 micro-F1=0.4242 macro-F1=0.2410\n",
            "Fold 4 Epoch 6: loss=0.8514 micro-F1=0.5198 macro-F1=0.4166\n",
            "Fold 4 Epoch 7: loss=0.6138 micro-F1=0.5746 macro-F1=0.4737\n",
            "Fold 4 Epoch 8: loss=0.4725 micro-F1=0.6616 macro-F1=0.4983\n",
            "Fold 4 Epoch 9: loss=0.3416 micro-F1=0.6789 macro-F1=0.5432\n",
            "Fold 4 Epoch 10: loss=0.2863 micro-F1=0.6839 macro-F1=0.5407\n",
            "Fold 4 Epoch 11: loss=0.2404 micro-F1=0.6988 macro-F1=0.5546\n",
            "Fold 4 Epoch 12: loss=0.2112 micro-F1=0.6785 macro-F1=0.5040\n",
            "Fold 4 Epoch 13: loss=0.1760 micro-F1=0.6627 macro-F1=0.4396\n",
            "Fold 4 Epoch 14: loss=0.1604 micro-F1=0.7010 macro-F1=0.5565\n",
            "Fold 4 Epoch 15: loss=0.1496 micro-F1=0.6784 macro-F1=0.5511\n",
            "Fold 4 Epoch 16: loss=0.1273 micro-F1=0.6826 macro-F1=0.5446\n",
            "Fold 4 Epoch 17: loss=0.1177 micro-F1=0.6530 macro-F1=0.4309\n",
            "Fold 4 Epoch 18: loss=0.1049 micro-F1=0.6926 macro-F1=0.4748\n",
            "Fold 4 Epoch 19: loss=0.1050 micro-F1=0.6944 macro-F1=0.5507\n",
            "Fold 4 Epoch 20: loss=0.0996 micro-F1=0.6762 macro-F1=0.4586\n",
            "Fold 4 done. Best val micro-F1 (fold): 0.7010\n",
            "\n",
            "=== FOLD 5/5 ===\n",
            "Fold 5 Epoch 1: loss=1.2635 micro-F1=0.2899 macro-F1=0.1561\n",
            "Fold 5 Epoch 2: loss=1.2086 micro-F1=0.3941 macro-F1=0.1654\n",
            "Fold 5 Epoch 3: loss=1.1690 micro-F1=0.2861 macro-F1=0.1813\n",
            "Fold 5 Epoch 4: loss=1.1637 micro-F1=0.3736 macro-F1=0.2568\n",
            "Fold 5 Epoch 5: loss=1.1399 micro-F1=0.4081 macro-F1=0.3472\n",
            "Fold 5 Epoch 6: loss=0.9331 micro-F1=0.5392 macro-F1=0.4034\n",
            "Fold 5 Epoch 7: loss=0.6653 micro-F1=0.5094 macro-F1=0.4119\n",
            "Fold 5 Epoch 8: loss=0.5322 micro-F1=0.6146 macro-F1=0.5008\n",
            "Fold 5 Epoch 9: loss=0.4322 micro-F1=0.6071 macro-F1=0.5227\n",
            "Fold 5 Epoch 10: loss=0.3432 micro-F1=0.6616 macro-F1=0.5127\n",
            "Fold 5 Epoch 11: loss=0.2862 micro-F1=0.6641 macro-F1=0.5544\n",
            "Fold 5 Epoch 12: loss=0.2441 micro-F1=0.6854 macro-F1=0.5460\n",
            "Fold 5 Epoch 13: loss=0.2165 micro-F1=0.6693 macro-F1=0.5533\n",
            "Fold 5 Epoch 14: loss=0.1815 micro-F1=0.6481 macro-F1=0.5315\n",
            "Fold 5 Epoch 15: loss=0.1907 micro-F1=0.6522 macro-F1=0.5307\n",
            "Fold 5 Epoch 16: loss=0.1640 micro-F1=0.6982 macro-F1=0.5690\n",
            "Fold 5 Epoch 17: loss=0.1574 micro-F1=0.6926 macro-F1=0.5716\n",
            "Fold 5 Epoch 18: loss=0.1308 micro-F1=0.7230 macro-F1=0.6001\n",
            "Fold 5 Epoch 19: loss=0.1256 micro-F1=0.6639 macro-F1=0.5080\n",
            "Fold 5 Epoch 20: loss=0.1237 micro-F1=0.6707 macro-F1=0.5321\n",
            "Fold 5 done. Best val micro-F1 (fold): 0.7230\n",
            "Saved OOF logits to /content/text_audio_oof_logits.npy\n",
            "\n",
            "Best global threshold: 0.50 | micro-F1: 0.7082 | macro-F1: 0.5776\n",
            "Updated summary CSV at /content/analysis_outputs/summary_metrics.csv\n",
            "Done (Text + Audio).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready cell: Text + Image baseline (TF-IDF + EfficientNet emb -> MLP)\n",
        "# Saves /content/text_image_oof_logits.npy and updates /content/analysis_outputs/summary_metrics.csv\n",
        "!pip install -q timm scikit-learn torchvision pillow tqdm\n",
        "\n",
        "import os, gc, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from PIL import Image\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "CSV_PATH = \"/content/clips_with_audio_clean2.csv\"\n",
        "IMG_COL_CAND = [\"Image_path_final\", \"Image_path\"]\n",
        "TEXT_COL_CAND = [\"Updated_Question\", \"Question_summ\"]\n",
        "LABEL_COL = \"probable_classes\"\n",
        "\n",
        "IMG_EMB_DIR = \"/content/efficientnet_img_embs\"\n",
        "os.makedirs(IMG_EMB_DIR, exist_ok=True)\n",
        "\n",
        "OUT_OOF = \"/content/text_image_oof_logits.npy\"\n",
        "SUMMARY_CSV = \"/content/analysis_outputs/summary_metrics.csv\"\n",
        "os.makedirs(os.path.dirname(SUMMARY_CSV), exist_ok=True)\n",
        "\n",
        "FOLDS = 5\n",
        "SEED = 42\n",
        "IMG_SIZE = 300\n",
        "TFIDF_DIM = 2000\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---------------- Load CSV & select columns ----------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Original rows:\", len(df))\n",
        "\n",
        "# pick text & image columns\n",
        "TEXT_COL = next((c for c in TEXT_COL_CAND if c in df.columns), None)\n",
        "IMG_COL = next((c for c in IMG_COL_CAND if c in df.columns), None)\n",
        "if TEXT_COL is None or IMG_COL is None:\n",
        "    raise ValueError(\"Text or Image column not found in CSV. Candidates: text %s image %s\" % (TEXT_COL_CAND, IMG_COL_CAND))\n",
        "\n",
        "# filter rows\n",
        "mask = df[IMG_COL].notna() & df[TEXT_COL].notna() & df[LABEL_COL].notna()\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Rows after filter:\", len(df))\n",
        "\n",
        "# ---------------- Multi-label matrix ----------------\n",
        "raw_labels = df[LABEL_COL].astype(str).tolist()\n",
        "all_labels = sorted({tok.strip() for s in raw_labels for tok in s.split(\",\") if tok.strip()})\n",
        "label_to_idx = {lab:i for i, lab in enumerate(all_labels)}\n",
        "N = len(df)\n",
        "print(\"Num labels:\", len(all_labels), \"Num samples:\", N)\n",
        "\n",
        "Y = np.zeros((N, len(all_labels)), dtype=np.int8)\n",
        "for i, s in enumerate(raw_labels):\n",
        "    for tok in s.split(\",\"):\n",
        "        t = tok.strip()\n",
        "        if t in label_to_idx:\n",
        "            Y[i, label_to_idx[t]] = 1\n",
        "print(\"Y shape:\", Y.shape)\n",
        "\n",
        "# save y_true if missing\n",
        "if not os.path.exists(\"/content/y_true.npy\"):\n",
        "    np.save(\"/content/y_true.npy\", Y.astype(np.int8))\n",
        "    print(\"Saved /content/y_true.npy\")\n",
        "else:\n",
        "    print(\"/content/y_true.npy exists; reusing it.\")\n",
        "\n",
        "# ---------------- fold assignments ----------------\n",
        "if os.path.exists(\"/content/fold_ids.npy\"):\n",
        "    fold_ids = np.load(\"/content/fold_ids.npy\")\n",
        "    print(\"Loaded existing /content/fold_ids.npy\")\n",
        "else:\n",
        "    print(\"Creating KFold assignments and saving /content/fold_ids.npy\")\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_ids = np.zeros(N, dtype=np.int32)\n",
        "    for fold, (_, vl_idx) in enumerate(kf.split(np.arange(N)), start=1):\n",
        "        fold_ids[vl_idx] = fold\n",
        "    np.save(\"/content/fold_ids.npy\", fold_ids)\n",
        "    print(\"Saved /content/fold_ids.npy\")\n",
        "\n",
        "# ---------------- TF-IDF ----------------\n",
        "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
        "tfidf = TfidfVectorizer(max_features=TFIDF_DIM, ngram_range=(1,2), min_df=1)\n",
        "X_text_sparse = tfidf.fit_transform(texts)\n",
        "text_dim = X_text_sparse.shape[1]\n",
        "X_text = X_text_sparse.toarray().astype(np.float32)\n",
        "print(\"TF-IDF dim:\", text_dim)\n",
        "\n",
        "# ---------------- Precompute EfficientNet image embeddings ----------------\n",
        "print(\"Preparing EfficientNet-B3 for image embedding extraction...\")\n",
        "img_model = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
        "# remove classifier\n",
        "if hasattr(img_model, \"classifier\"):\n",
        "    in_feats = img_model.classifier.in_features\n",
        "    img_model.classifier = nn.Identity()\n",
        "elif hasattr(img_model, \"fc\"):\n",
        "    in_feats = img_model.fc.in_features\n",
        "    img_model.fc = nn.Identity()\n",
        "else:\n",
        "    # fallback\n",
        "    in_feats = None\n",
        "\n",
        "img_model.eval().to(DEVICE)\n",
        "\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def build_img_emb(i, img_path):\n",
        "    outp = os.path.join(IMG_EMB_DIR, f\"emb_{i}.npy\")\n",
        "    if os.path.exists(outp):\n",
        "        return\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = eval_tf(img).unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            feat = img_model(img).cpu().numpy().squeeze().astype(np.float32)\n",
        "        np.save(outp, feat)\n",
        "    except Exception as e:\n",
        "        # if missing/bad image, save zeros\n",
        "        if in_feats is None:\n",
        "            # try running dummy to determine dim\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(DEVICE)\n",
        "                dim = img_model(dummy).cpu().numpy().shape[1]\n",
        "        else:\n",
        "            dim = in_feats\n",
        "        np.save(outp, np.zeros(dim, dtype=np.float32))\n",
        "        print(f\"[WARN] failed to process image {img_path}: {e}. Saved zeros.\")\n",
        "\n",
        "print(\"Precomputing image embeddings (skips existing)...\")\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    build_img_emb(i, row[IMG_COL])\n",
        "print(\"Image embeddings ready in\", IMG_EMB_DIR)\n",
        "\n",
        "# load image embeddings matrix\n",
        "sample_emb = np.load(os.path.join(IMG_EMB_DIR, \"emb_0.npy\"))\n",
        "img_dim = sample_emb.shape[0]\n",
        "X_img = np.zeros((N, img_dim), dtype=np.float32)\n",
        "for i in range(N):\n",
        "    p = os.path.join(IMG_EMB_DIR, f\"emb_{i}.npy\")\n",
        "    if os.path.exists(p):\n",
        "        X_img[i] = np.load(p).astype(np.float32)\n",
        "    else:\n",
        "        X_img[i] = np.zeros(img_dim, dtype=np.float32)\n",
        "print(\"Loaded X_img shape:\", X_img.shape)\n",
        "\n",
        "# ---------------- Build fused features: text + image ----------------\n",
        "X_fused = np.concatenate([X_text, X_img], axis=1)\n",
        "fusion_dim = X_fused.shape[1]\n",
        "print(\"Fused input dim:\", fusion_dim)\n",
        "\n",
        "# ---------------- Simple MLP classifier ----------------\n",
        "class FusedNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_labels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, n_labels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def eval_f1(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n",
        "\n",
        "# ---------------- 5-fold training using fold_ids ----------------\n",
        "oof_logits = np.zeros((N, len(all_labels)), dtype=np.float32)\n",
        "\n",
        "for fold in range(1, FOLDS+1):\n",
        "    print(f\"\\n=== FOLD {fold}/{FOLDS} ===\")\n",
        "    vl_idx = np.where(fold_ids == fold)[0]\n",
        "    tr_idx = np.where(fold_ids != fold)[0]\n",
        "\n",
        "    X_tr = torch.tensor(X_fused[tr_idx]).float()\n",
        "    Y_tr = torch.tensor(Y[tr_idx]).float()\n",
        "    X_vl = torch.tensor(X_fused[vl_idx]).float()\n",
        "    Y_vl = torch.tensor(Y[vl_idx]).float()\n",
        "\n",
        "    tr_dataset = torch.utils.data.TensorDataset(X_tr, Y_tr)\n",
        "    vl_dataset = torch.utils.data.TensorDataset(X_vl, Y_vl)\n",
        "    tr_loader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    vl_loader = DataLoader(vl_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = FusedNet(fusion_dim, len(all_labels)).to(DEVICE)\n",
        "\n",
        "    # pos_weight from training labels\n",
        "    pos = Y_tr.numpy().sum(axis=0) + 1e-6\n",
        "    neg = Y_tr.shape[0] - pos + 1e-6\n",
        "    pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_micro = 0.0\n",
        "    best_logits_fold = None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in tr_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        # validation\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_y = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in vl_loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                out = model(xb).cpu().numpy()\n",
        "                all_logits.append(out)\n",
        "                all_y.append(yb.numpy())\n",
        "        all_logits = np.vstack(all_logits)\n",
        "        all_y = np.vstack(all_y)\n",
        "        probs = 1 / (1 + np.exp(-all_logits))\n",
        "        micro, macro = eval_f1(all_y, probs, thr=0.5)\n",
        "        print(f\"Fold {fold} Epoch {ep}: loss={running_loss/len(tr_loader):.4f} micro-F1={micro:.4f} macro-F1={macro:.4f}\")\n",
        "        if micro > best_micro + 1e-6:\n",
        "            best_micro = micro\n",
        "            best_logits_fold = all_logits.copy()\n",
        "\n",
        "    if best_logits_fold is None:\n",
        "        best_logits_fold = all_logits.copy()\n",
        "    oof_logits[vl_idx] = best_logits_fold\n",
        "    print(f\"Fold {fold} done. Best fold micro-F1: {best_micro:.4f}\")\n",
        "\n",
        "    del model, optimizer, tr_loader, vl_loader\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# ---------------- global threshold search and save ----------------\n",
        "np.save(OUT_OOF, oof_logits)\n",
        "print(\"Saved OOF logits to\", OUT_OOF)\n",
        "\n",
        "oof_probs = 1 / (1 + np.exp(-oof_logits))\n",
        "best_thr = 0.5; best_micro = 0.0; best_macro = 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 51):\n",
        "    micro, macro = eval_f1(Y, oof_probs, thr=thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro, best_macro, best_thr = micro, macro, thr\n",
        "\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {best_micro:.4f} | macro-F1: {best_macro:.4f}\")\n",
        "\n",
        "# ---------------- update summary CSV ----------------\n",
        "row = {\"model\":\"text+image\", \"micro_f1\":float(best_micro), \"macro_f1\":float(best_macro), \"threshold\":float(best_thr)}\n",
        "if os.path.exists(SUMMARY_CSV):\n",
        "    df_sum = pd.read_csv(SUMMARY_CSV)\n",
        "    df_sum = df_sum[df_sum.model != \"text+image\"]\n",
        "    df_sum = pd.concat([df_sum, pd.DataFrame([row])], ignore_index=True)\n",
        "else:\n",
        "    df_sum = pd.DataFrame([row])\n",
        "df_sum.to_csv(SUMMARY_CSV, index=False)\n",
        "print(\"Updated summary CSV at\", SUMMARY_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MIBPzuG5EH4",
        "outputId": "0729fccf-0e75-472a-d7da-2ab5043c1e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Original rows: 404\n",
            "Rows after filter: 404\n",
            "Num labels: 18 Num samples: 404\n",
            "Y shape: (404, 18)\n",
            "/content/y_true.npy exists; reusing it.\n",
            "Loaded existing /content/fold_ids.npy\n",
            "TF-IDF dim: 2000\n",
            "Preparing EfficientNet-B3 for image embedding extraction...\n",
            "Precomputing image embeddings (skips existing)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404/404 [00:00<00:00, 19421.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image embeddings ready in /content/efficientnet_img_embs\n",
            "Loaded X_img shape: (404, 1536)\n",
            "Fused input dim: 3536\n",
            "\n",
            "=== FOLD 1/5 ===\n",
            "Fold 1 Epoch 1: loss=0.9285 micro-F1=0.4776 macro-F1=0.4015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 2: loss=0.6931 micro-F1=0.5039 macro-F1=0.4204\n",
            "Fold 1 Epoch 3: loss=0.5961 micro-F1=0.5092 macro-F1=0.4225\n",
            "Fold 1 Epoch 4: loss=0.5738 micro-F1=0.5140 macro-F1=0.4196\n",
            "Fold 1 Epoch 5: loss=0.5430 micro-F1=0.5106 macro-F1=0.4202\n",
            "Fold 1 Epoch 6: loss=0.4886 micro-F1=0.5136 macro-F1=0.4246\n",
            "Fold 1 Epoch 7: loss=0.4916 micro-F1=0.5116 macro-F1=0.4211\n",
            "Fold 1 Epoch 8: loss=0.4717 micro-F1=0.5055 macro-F1=0.4198\n",
            "Fold 1 Epoch 9: loss=0.4574 micro-F1=0.5337 macro-F1=0.4359\n",
            "Fold 1 Epoch 10: loss=0.4270 micro-F1=0.5277 macro-F1=0.4325\n",
            "Fold 1 Epoch 11: loss=0.4818 micro-F1=0.5429 macro-F1=0.4361\n",
            "Fold 1 Epoch 12: loss=0.4167 micro-F1=0.5369 macro-F1=0.4359\n",
            "Fold 1 Epoch 13: loss=0.4054 micro-F1=0.5329 macro-F1=0.4364\n",
            "Fold 1 Epoch 14: loss=0.4809 micro-F1=0.5452 macro-F1=0.4458\n",
            "Fold 1 Epoch 15: loss=0.4003 micro-F1=0.5367 macro-F1=0.4392\n",
            "Fold 1 Epoch 16: loss=0.3703 micro-F1=0.5714 macro-F1=0.4507\n",
            "Fold 1 Epoch 17: loss=0.3444 micro-F1=0.5632 macro-F1=0.4575\n",
            "Fold 1 Epoch 18: loss=0.3251 micro-F1=0.5373 macro-F1=0.4415\n",
            "Fold 1 Epoch 19: loss=0.3202 micro-F1=0.5630 macro-F1=0.4572\n",
            "Fold 1 Epoch 20: loss=0.3137 micro-F1=0.5723 macro-F1=0.4670\n",
            "Fold 1 done. Best fold micro-F1: 0.5723\n",
            "\n",
            "=== FOLD 2/5 ===\n",
            "Fold 2 Epoch 1: loss=0.9787 micro-F1=0.5483 macro-F1=0.4551\n",
            "Fold 2 Epoch 2: loss=0.7571 micro-F1=0.5917 macro-F1=0.4805\n",
            "Fold 2 Epoch 3: loss=0.6569 micro-F1=0.6132 macro-F1=0.4846\n",
            "Fold 2 Epoch 4: loss=0.5928 micro-F1=0.5916 macro-F1=0.4855\n",
            "Fold 2 Epoch 5: loss=0.5801 micro-F1=0.5898 macro-F1=0.4825\n",
            "Fold 2 Epoch 6: loss=0.6078 micro-F1=0.6246 macro-F1=0.5002\n",
            "Fold 2 Epoch 7: loss=0.5384 micro-F1=0.5942 macro-F1=0.4770\n",
            "Fold 2 Epoch 8: loss=0.5399 micro-F1=0.6258 macro-F1=0.5087\n",
            "Fold 2 Epoch 9: loss=0.5207 micro-F1=0.6222 macro-F1=0.5041\n",
            "Fold 2 Epoch 10: loss=0.5068 micro-F1=0.6133 macro-F1=0.4996\n",
            "Fold 2 Epoch 11: loss=0.4987 micro-F1=0.6178 macro-F1=0.4975\n",
            "Fold 2 Epoch 12: loss=0.4959 micro-F1=0.6262 macro-F1=0.5045\n",
            "Fold 2 Epoch 13: loss=0.4347 micro-F1=0.6342 macro-F1=0.5142\n",
            "Fold 2 Epoch 14: loss=0.4362 micro-F1=0.6317 macro-F1=0.5093\n",
            "Fold 2 Epoch 15: loss=0.4369 micro-F1=0.6317 macro-F1=0.5116\n",
            "Fold 2 Epoch 16: loss=0.4444 micro-F1=0.6678 macro-F1=0.5502\n",
            "Fold 2 Epoch 17: loss=0.3877 micro-F1=0.6473 macro-F1=0.5143\n",
            "Fold 2 Epoch 18: loss=0.3644 micro-F1=0.6635 macro-F1=0.5448\n",
            "Fold 2 Epoch 19: loss=0.4309 micro-F1=0.6550 macro-F1=0.5311\n",
            "Fold 2 Epoch 20: loss=0.3404 micro-F1=0.6765 macro-F1=0.5541\n",
            "Fold 2 done. Best fold micro-F1: 0.6765\n",
            "\n",
            "=== FOLD 3/5 ===\n",
            "Fold 3 Epoch 1: loss=1.0162 micro-F1=0.4806 macro-F1=0.4098\n",
            "Fold 3 Epoch 2: loss=0.7610 micro-F1=0.5545 macro-F1=0.4525\n",
            "Fold 3 Epoch 3: loss=0.6225 micro-F1=0.5812 macro-F1=0.4871\n",
            "Fold 3 Epoch 4: loss=0.5924 micro-F1=0.5730 macro-F1=0.4819\n",
            "Fold 3 Epoch 5: loss=0.5477 micro-F1=0.5877 macro-F1=0.4913\n",
            "Fold 3 Epoch 6: loss=0.5284 micro-F1=0.6087 macro-F1=0.5072\n",
            "Fold 3 Epoch 7: loss=0.5299 micro-F1=0.6003 macro-F1=0.5071\n",
            "Fold 3 Epoch 8: loss=0.5169 micro-F1=0.5952 macro-F1=0.4962\n",
            "Fold 3 Epoch 9: loss=0.4902 micro-F1=0.6094 macro-F1=0.5023\n",
            "Fold 3 Epoch 10: loss=0.5034 micro-F1=0.6121 macro-F1=0.5138\n",
            "Fold 3 Epoch 11: loss=0.4708 micro-F1=0.6406 macro-F1=0.5364\n",
            "Fold 3 Epoch 12: loss=0.4461 micro-F1=0.6053 macro-F1=0.5122\n",
            "Fold 3 Epoch 13: loss=0.4336 micro-F1=0.6229 macro-F1=0.5248\n",
            "Fold 3 Epoch 14: loss=0.4387 micro-F1=0.6538 macro-F1=0.5613\n",
            "Fold 3 Epoch 15: loss=0.4162 micro-F1=0.6228 macro-F1=0.5216\n",
            "Fold 3 Epoch 16: loss=0.3996 micro-F1=0.6656 macro-F1=0.5692\n",
            "Fold 3 Epoch 17: loss=0.4062 micro-F1=0.6560 macro-F1=0.5526\n",
            "Fold 3 Epoch 18: loss=0.3973 micro-F1=0.6289 macro-F1=0.5274\n",
            "Fold 3 Epoch 19: loss=0.3494 micro-F1=0.6478 macro-F1=0.5228\n",
            "Fold 3 Epoch 20: loss=0.2995 micro-F1=0.6723 macro-F1=0.5290\n",
            "Fold 3 done. Best fold micro-F1: 0.6723\n",
            "\n",
            "=== FOLD 4/5 ===\n",
            "Fold 4 Epoch 1: loss=0.9973 micro-F1=0.5101 macro-F1=0.4254\n",
            "Fold 4 Epoch 2: loss=0.7194 micro-F1=0.5806 macro-F1=0.4752\n",
            "Fold 4 Epoch 3: loss=0.6514 micro-F1=0.5775 macro-F1=0.4709\n",
            "Fold 4 Epoch 4: loss=0.6108 micro-F1=0.5964 macro-F1=0.4860\n",
            "Fold 4 Epoch 5: loss=0.5727 micro-F1=0.6203 macro-F1=0.5048\n",
            "Fold 4 Epoch 6: loss=0.5874 micro-F1=0.6156 macro-F1=0.4907\n",
            "Fold 4 Epoch 7: loss=0.5067 micro-F1=0.6185 macro-F1=0.5105\n",
            "Fold 4 Epoch 8: loss=0.5556 micro-F1=0.6254 macro-F1=0.5075\n",
            "Fold 4 Epoch 9: loss=0.5042 micro-F1=0.5932 macro-F1=0.4758\n",
            "Fold 4 Epoch 10: loss=0.4980 micro-F1=0.6171 macro-F1=0.5087\n",
            "Fold 4 Epoch 11: loss=0.4840 micro-F1=0.6535 macro-F1=0.5409\n",
            "Fold 4 Epoch 12: loss=0.4720 micro-F1=0.6166 macro-F1=0.5030\n",
            "Fold 4 Epoch 13: loss=0.4716 micro-F1=0.6339 macro-F1=0.5135\n",
            "Fold 4 Epoch 14: loss=0.4387 micro-F1=0.6361 macro-F1=0.5294\n",
            "Fold 4 Epoch 15: loss=0.4295 micro-F1=0.6533 macro-F1=0.5375\n",
            "Fold 4 Epoch 16: loss=0.4081 micro-F1=0.6562 macro-F1=0.5566\n",
            "Fold 4 Epoch 17: loss=0.4303 micro-F1=0.6269 macro-F1=0.5418\n",
            "Fold 4 Epoch 18: loss=0.3653 micro-F1=0.6667 macro-F1=0.5360\n",
            "Fold 4 Epoch 19: loss=0.3903 micro-F1=0.6481 macro-F1=0.5365\n",
            "Fold 4 Epoch 20: loss=0.3423 micro-F1=0.6586 macro-F1=0.5463\n",
            "Fold 4 done. Best fold micro-F1: 0.6667\n",
            "\n",
            "=== FOLD 5/5 ===\n",
            "Fold 5 Epoch 1: loss=0.9656 micro-F1=0.5463 macro-F1=0.4447\n",
            "Fold 5 Epoch 2: loss=0.7049 micro-F1=0.5455 macro-F1=0.4539\n",
            "Fold 5 Epoch 3: loss=0.6459 micro-F1=0.5714 macro-F1=0.4797\n",
            "Fold 5 Epoch 4: loss=0.6058 micro-F1=0.5787 macro-F1=0.4769\n",
            "Fold 5 Epoch 5: loss=0.5773 micro-F1=0.5672 macro-F1=0.4806\n",
            "Fold 5 Epoch 6: loss=0.5476 micro-F1=0.5860 macro-F1=0.4922\n",
            "Fold 5 Epoch 7: loss=0.5327 micro-F1=0.5854 macro-F1=0.4859\n",
            "Fold 5 Epoch 8: loss=0.5256 micro-F1=0.5811 macro-F1=0.4781\n",
            "Fold 5 Epoch 9: loss=0.5299 micro-F1=0.5847 macro-F1=0.4854\n",
            "Fold 5 Epoch 10: loss=0.4646 micro-F1=0.5927 macro-F1=0.4970\n",
            "Fold 5 Epoch 11: loss=0.5034 micro-F1=0.6047 macro-F1=0.4978\n",
            "Fold 5 Epoch 12: loss=0.4715 micro-F1=0.5927 macro-F1=0.4910\n",
            "Fold 5 Epoch 13: loss=0.4582 micro-F1=0.5949 macro-F1=0.4907\n",
            "Fold 5 Epoch 14: loss=0.4337 micro-F1=0.5942 macro-F1=0.4856\n",
            "Fold 5 Epoch 15: loss=0.4605 micro-F1=0.5881 macro-F1=0.4869\n",
            "Fold 5 Epoch 16: loss=0.4312 micro-F1=0.6019 macro-F1=0.4997\n",
            "Fold 5 Epoch 17: loss=0.4059 micro-F1=0.6070 macro-F1=0.4951\n",
            "Fold 5 Epoch 18: loss=0.3861 micro-F1=0.6023 macro-F1=0.4859\n",
            "Fold 5 Epoch 19: loss=0.3913 micro-F1=0.6020 macro-F1=0.4910\n",
            "Fold 5 Epoch 20: loss=0.3524 micro-F1=0.6316 macro-F1=0.5034\n",
            "Fold 5 done. Best fold micro-F1: 0.6316\n",
            "Saved OOF logits to /content/text_image_oof_logits.npy\n",
            "\n",
            "Best global threshold: 0.80 | micro-F1: 0.6768 | macro-F1: 0.5443\n",
            "Updated summary CSV at /content/analysis_outputs/summary_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Image + Audio baseline (EfficientNet + wav2vec2) - Colab cell ==========\n",
        "!pip install -q timm transformers librosa soundfile scikit-learn\n",
        "\n",
        "import os, gc, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "# ----------------- Config -----------------\n",
        "CSV_PATH   = \"/content/clips_with_audio_clean2.csv\"\n",
        "IMG_COL    = \"Image_path_final\"   # adjust if needed\n",
        "AUDIO_COL  = \"audio_path\"\n",
        "LABEL_COL  = \"probable_classes\"\n",
        "TEXT_COLS  = [\"Updated_Question\", \"Question_summ\"]  # not used here but kept\n",
        "\n",
        "FOLDS      = 5\n",
        "SEED       = 42\n",
        "IMG_SIZE   = 300\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS     = 20\n",
        "TARGET_SR  = 16000\n",
        "AUDIO_SEC  = 3   # seconds to pad/trim to\n",
        "EMB_DIR_AUD = \"/content/wav2vec_multi_embs\"   # audio emb cache\n",
        "EMB_DIR_IMG = \"/content/efficientnet_img_embs\" # image emb cache\n",
        "OUT_OOF    = \"/content/image_audio_oof_logits.npy\"\n",
        "SUMMARY_CSV = \"/content/analysis_outputs/summary_metrics.csv\"\n",
        "os.makedirs(EMB_DIR_AUD, exist_ok=True)\n",
        "os.makedirs(EMB_DIR_IMG, exist_ok=True)\n",
        "os.makedirs(os.path.dirname(SUMMARY_CSV), exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ----------------- Load CSV and build labels -----------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Original rows:\", len(df))\n",
        "mask = df[IMG_COL].notna() & df[AUDIO_COL].notna() & df[LABEL_COL].notna()\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Rows after basic filter:\", len(df))\n",
        "\n",
        "# extract label set and multi-hot Y\n",
        "raw_labels = df[LABEL_COL].astype(str).tolist()\n",
        "all_labels = sorted({lab.strip() for r in raw_labels for lab in r.split(\",\") if lab.strip()})\n",
        "label_to_idx = {l:i for i,l in enumerate(all_labels)}\n",
        "print(\"Num labels:\", len(all_labels))\n",
        "\n",
        "Y = np.zeros((len(df), len(all_labels)), dtype=np.float32)\n",
        "for i, r in enumerate(raw_labels):\n",
        "    for lab in r.split(\",\"):\n",
        "        lab = lab.strip()\n",
        "        if lab in label_to_idx:\n",
        "            Y[i, label_to_idx[lab]] = 1.0\n",
        "\n",
        "# save y_true if not present\n",
        "if not os.path.exists(\"/content/y_true.npy\"):\n",
        "    np.save(\"/content/y_true.npy\", Y)\n",
        "    print(\"Saved /content/y_true.npy\")\n",
        "else:\n",
        "    print(\"Using existing /content/y_true.npy\")\n",
        "\n",
        "df[\"row_id\"] = np.arange(len(df))\n",
        "\n",
        "# primary label for stratification\n",
        "primary = Y.argmax(axis=1)\n",
        "\n",
        "# ----------------- Prepare wav2vec2 (audio) embeddings -----------------\n",
        "print(\"Loading wav2vec2 processor + model...\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device).eval()\n",
        "aud_emb_dim = wav2vec_model.config.hidden_size\n",
        "print(\"wav2vec emb dim:\", aud_emb_dim)\n",
        "\n",
        "def extract_audio_emb(rid, audio_path):\n",
        "    out = os.path.join(EMB_DIR_AUD, f\"emb_{rid}.npy\")\n",
        "    if os.path.exists(out):\n",
        "        return\n",
        "    try:\n",
        "        y, sr = sf.read(audio_path, dtype=\"float32\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=None)\n",
        "            y = y.astype(\"float32\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] audio load fail {audio_path}: {e}\")\n",
        "            np.save(out, np.zeros(aud_emb_dim, dtype=np.float32))\n",
        "            return\n",
        "    if y.ndim > 1:\n",
        "        y = np.mean(y, axis=1)\n",
        "    if sr != TARGET_SR:\n",
        "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
        "        sr = TARGET_SR\n",
        "    wanted = TARGET_SR * AUDIO_SEC\n",
        "    if len(y) < wanted:\n",
        "        y = np.pad(y, (0, wanted-len(y)))\n",
        "    else:\n",
        "        y = y[:wanted]\n",
        "    with torch.no_grad():\n",
        "        inp = processor(y, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True)\n",
        "        outm = wav2vec_model(inp.input_values.to(device))\n",
        "        emb = outm.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy().astype(np.float32)\n",
        "    np.save(out, emb)\n",
        "\n",
        "print(\"Precomputing audio embeddings...\")\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    extract_audio_emb(int(row.row_id), row[AUDIO_COL])\n",
        "print(\"Audio embeddings ready in\", EMB_DIR_AUD)\n",
        "\n",
        "# ----------------- Prepare image embeddings (EfficientNet-B3) -----------------\n",
        "print(\"Preparing EfficientNet-B3 for image embeddings...\")\n",
        "img_model = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
        "# remove classifier\n",
        "if hasattr(img_model, \"classifier\"):\n",
        "    in_feats = img_model.classifier.in_features\n",
        "    img_model.classifier = nn.Identity()\n",
        "elif hasattr(img_model, \"fc\"):\n",
        "    in_feats = img_model.fc.in_features\n",
        "    img_model.fc = nn.Identity()\n",
        "else:\n",
        "    # fallback: will infer by running a dummy\n",
        "    in_feats = None\n",
        "\n",
        "img_model = img_model.to(device).eval()\n",
        "img_emb_dim = in_feats if in_feats is not None else 1536\n",
        "print(\"EfficientNet image emb dim:\", img_emb_dim)\n",
        "\n",
        "from torchvision import transforms\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.CenterCrop((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def extract_img_emb(rid, img_path):\n",
        "    out = os.path.join(EMB_DIR_IMG, f\"img_emb_{rid}.npy\")\n",
        "    if os.path.exists(out):\n",
        "        return\n",
        "    try:\n",
        "        im = Image.open(img_path).convert(\"RGB\")\n",
        "        x = img_tf(im).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat = img_model(x).cpu().numpy().squeeze(0).astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] image load fail {img_path}: {e}\")\n",
        "        feat = np.zeros(img_emb_dim, dtype=np.float32)\n",
        "    np.save(out, feat)\n",
        "\n",
        "print(\"Precomputing image embeddings (skips existing)...\")\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    extract_img_emb(int(row.row_id), row[IMG_COL])\n",
        "print(\"Image embeddings ready in\", EMB_DIR_IMG)\n",
        "\n",
        "# ----------------- Dataset for Image+Audio -----------------\n",
        "class ImgAudDataset(Dataset):\n",
        "    def __init__(self, df_sub, emb_dir_img, emb_dir_aud, Y_full):\n",
        "        self.df = df_sub.reset_index(drop=True)\n",
        "        self.emb_dir_img = emb_dir_img\n",
        "        self.emb_dir_aud = emb_dir_aud\n",
        "        self.Y_full = Y_full\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        rid = int(self.df.loc[idx, \"row_id\"])\n",
        "        img_emb = np.load(os.path.join(self.emb_dir_img, f\"img_emb_{rid}.npy\")).astype(np.float32)\n",
        "        aud_emb = np.load(os.path.join(self.emb_dir_aud, f\"emb_{rid}.npy\")).astype(np.float32)\n",
        "        y = self.Y_full[rid]\n",
        "        return img_emb, aud_emb, y\n",
        "\n",
        "# ----------------- Fusion model (ImageEmb + AudioEmb -> logits) -----------------\n",
        "class ImgAudNet(nn.Module):\n",
        "    def __init__(self, img_dim, aud_dim, n_labels):\n",
        "        super().__init__()\n",
        "        self.img_proj = nn.Sequential(nn.Linear(img_dim, 512), nn.ReLU(), nn.LayerNorm(512))\n",
        "        self.aud_proj = nn.Sequential(nn.Linear(aud_dim, 512), nn.ReLU(), nn.LayerNorm(512))\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512+512, 512), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512, n_labels)\n",
        "        )\n",
        "    def forward(self, img_emb, aud_emb):\n",
        "        x1 = self.img_proj(img_emb)\n",
        "        x2 = self.aud_proj(aud_emb)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        return self.head(x)\n",
        "\n",
        "# ----------------- Build / load fold assignments -----------------\n",
        "if os.path.exists(\"/content/fold_ids.npy\"):\n",
        "    fold_ids = np.load(\"/content/fold_ids.npy\")\n",
        "    print(\"Loaded existing /content/fold_ids.npy\")\n",
        "else:\n",
        "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_ids = np.zeros(len(df), dtype=np.int32)\n",
        "    for fold, (_, vl) in enumerate(skf.split(df.index, primary), start=1):\n",
        "        fold_ids[vl] = fold\n",
        "    np.save(\"/content/fold_ids.npy\", fold_ids)\n",
        "    print(\"Saved /content/fold_ids.npy\")\n",
        "\n",
        "# ----------------- 5-Fold training / OOF -----------------\n",
        "n_labels = len(all_labels)\n",
        "oof_logits = np.zeros((len(df), n_labels), dtype=np.float32)\n",
        "\n",
        "for fold in range(1, FOLDS+1):\n",
        "    print(\"\\n=== FOLD\", fold, \"/\", FOLDS, \"===\")\n",
        "    tr_idx = np.where(fold_ids != fold)[0]\n",
        "    vl_idx = np.where(fold_ids == fold)[0]\n",
        "    tr_df = df.iloc[tr_idx].reset_index(drop=True)\n",
        "    vl_df = df.iloc[vl_idx].reset_index(drop=True)\n",
        "\n",
        "    tr_ds = ImgAudDataset(tr_df, EMB_DIR_IMG, EMB_DIR_AUD, Y)\n",
        "    vl_ds = ImgAudDataset(vl_df, EMB_DIR_IMG, EMB_DIR_AUD, Y)\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    vl_loader = DataLoader(vl_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # pos weight from training labels (for BCEWithLogits)\n",
        "    pos = Y[tr_idx].sum(axis=0) + 1e-6\n",
        "    neg = len(tr_idx) - pos + 1e-6\n",
        "    pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=device)\n",
        "\n",
        "    model = ImgAudNet(img_emb_dim, aud_emb_dim, n_labels).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "    best_micro = 0.0\n",
        "    best_path = f\"/content/image_audio_fold{fold}.pth\"\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for img_e, aud_e, y in tr_loader:\n",
        "            img_e = img_e.to(device)\n",
        "            aud_e = aud_e.to(device)\n",
        "            y = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(img_e, aud_e)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        # validation\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_y = []\n",
        "        with torch.no_grad():\n",
        "            for img_e, aud_e, y in vl_loader:\n",
        "                img_e = img_e.to(device)\n",
        "                aud_e = aud_e.to(device)\n",
        "                out = model(img_e, aud_e).cpu().numpy()\n",
        "                all_logits.append(out)\n",
        "                all_y.append(y)\n",
        "        all_logits = np.vstack(all_logits)\n",
        "        all_y = np.vstack(all_y)\n",
        "        probs = 1/(1+np.exp(-all_logits))\n",
        "        micro = f1_score(all_y, (probs>=0.5).astype(int), average=\"micro\", zero_division=0)\n",
        "        macro = f1_score(all_y, (probs>=0.5).astype(int), average=\"macro\", zero_division=0)\n",
        "        print(f\"Fold {fold} Epoch {ep}: loss={total_loss/len(tr_loader):.4f} micro-F1={micro:.4f} macro-F1={macro:.4f}\")\n",
        "        if micro > best_micro + 1e-6:\n",
        "            best_micro = micro\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(\" -> saved best model for fold\", fold, \" (micro-F1=\", best_micro, \")\")\n",
        "\n",
        "    # load best and write OOF logits for this fold\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    model.eval()\n",
        "    probs_fold = []\n",
        "    with torch.no_grad():\n",
        "        for img_e, aud_e, y in vl_loader:\n",
        "            img_e = img_e.to(device)\n",
        "            aud_e = aud_e.to(device)\n",
        "            out = model(img_e, aud_e)\n",
        "            probs_fold.append(torch.sigmoid(out).cpu().numpy())\n",
        "    probs_fold = np.vstack(probs_fold)\n",
        "    oof_logits[vl_idx] = probs_fold\n",
        "    print(\"Fold\", fold, \"done. Best micro-F1:\", best_micro)\n",
        "    del model, optimizer, tr_loader, vl_loader\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# ----------------- global threshold search & metrics -----------------\n",
        "def eval_f1(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n",
        "\n",
        "best_thr, best_micro, best_macro = 0.5, 0.0, 0.0\n",
        "for thr in np.linspace(0.3, 0.8, 11):\n",
        "    micro, macro = eval_f1(Y, oof_logits, thr=thr)\n",
        "    if micro > best_micro:\n",
        "        best_micro, best_macro, best_thr = micro, macro, thr\n",
        "\n",
        "final_micro, final_macro = eval_f1(Y, oof_logits, thr=best_thr)\n",
        "print(f\"\\nBest global threshold: {best_thr:.2f} | micro-F1: {final_micro:.4f} | macro-F1: {final_macro:.4f}\")\n",
        "\n",
        "# save OOF logits\n",
        "np.save(OUT_OOF, oof_logits)\n",
        "print(\"Saved OOF logits to\", OUT_OOF)\n",
        "\n",
        "# update summary CSV\n",
        "row = {\"model\":\"image+audio\", \"micro_f1\":float(final_micro), \"macro_f1\":float(final_macro), \"threshold\":float(best_thr)}\n",
        "if os.path.exists(SUMMARY_CSV):\n",
        "    df_sum = pd.read_csv(SUMMARY_CSV)\n",
        "    df_sum = df_sum[df_sum.model != \"image+audio\"]\n",
        "    df_sum = pd.concat([df_sum, pd.DataFrame([row])], ignore_index=True)\n",
        "else:\n",
        "    df_sum = pd.DataFrame([row])\n",
        "df_sum.to_csv(SUMMARY_CSV, index=False)\n",
        "print(\"Updated summary CSV at\", SUMMARY_CSV)\n",
        "\n",
        "# final housekeeping\n",
        "np.save(\"/content/y_true.npy\", Y)\n",
        "np.save(\"/content/fold_ids.npy\", fold_ids)\n",
        "print(\"Saved /content/y_true.npy and /content/fold_ids.npy (if replaced).\")\n",
        "print(\"Done (Image + Audio).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fbdzHVj7NfO",
        "outputId": "08dba14f-27f9-4990-a86e-4d1eff4c14c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Original rows: 404\n",
            "Rows after basic filter: 404\n",
            "Num labels: 18\n",
            "Using existing /content/y_true.npy\n",
            "Loading wav2vec2 processor + model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2vec emb dim: 768\n",
            "Precomputing audio embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404/404 [00:12<00:00, 32.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio embeddings ready in /content/wav2vec_multi_embs\n",
            "Preparing EfficientNet-B3 for image embeddings...\n",
            "EfficientNet image emb dim: 1536\n",
            "Precomputing image embeddings (skips existing)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404/404 [00:40<00:00,  9.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image embeddings ready in /content/efficientnet_img_embs\n",
            "Loaded existing /content/fold_ids.npy\n",
            "\n",
            "=== FOLD 1 / 5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 1: loss=0.9739 micro-F1=0.4834 macro-F1=0.4003\n",
            " -> saved best model for fold 1  (micro-F1= 0.4833759590792839 )\n",
            "Fold 1 Epoch 2: loss=0.7142 micro-F1=0.5029 macro-F1=0.4156\n",
            " -> saved best model for fold 1  (micro-F1= 0.5028735632183908 )\n",
            "Fold 1 Epoch 3: loss=0.6460 micro-F1=0.4798 macro-F1=0.4083\n",
            "Fold 1 Epoch 4: loss=0.5724 micro-F1=0.4879 macro-F1=0.4099\n",
            "Fold 1 Epoch 5: loss=0.5271 micro-F1=0.5306 macro-F1=0.4415\n",
            " -> saved best model for fold 1  (micro-F1= 0.5306122448979592 )\n",
            "Fold 1 Epoch 6: loss=0.5318 micro-F1=0.5247 macro-F1=0.4217\n",
            "Fold 1 Epoch 7: loss=0.4910 micro-F1=0.5324 macro-F1=0.4297\n",
            " -> saved best model for fold 1  (micro-F1= 0.5323529411764706 )\n",
            "Fold 1 Epoch 8: loss=0.4845 micro-F1=0.5579 macro-F1=0.4513\n",
            " -> saved best model for fold 1  (micro-F1= 0.5579119086460033 )\n",
            "Fold 1 Epoch 9: loss=0.5102 micro-F1=0.5311 macro-F1=0.4272\n",
            "Fold 1 Epoch 10: loss=0.4527 micro-F1=0.5358 macro-F1=0.4376\n",
            "Fold 1 Epoch 11: loss=0.4728 micro-F1=0.5130 macro-F1=0.4316\n",
            "Fold 1 Epoch 12: loss=0.4283 micro-F1=0.5552 macro-F1=0.4505\n",
            "Fold 1 Epoch 13: loss=0.3971 micro-F1=0.5621 macro-F1=0.4403\n",
            " -> saved best model for fold 1  (micro-F1= 0.5620915032679739 )\n",
            "Fold 1 Epoch 14: loss=0.3642 micro-F1=0.5785 macro-F1=0.4813\n",
            " -> saved best model for fold 1  (micro-F1= 0.578538102643857 )\n",
            "Fold 1 Epoch 15: loss=0.3406 micro-F1=0.5559 macro-F1=0.4525\n",
            "Fold 1 Epoch 16: loss=0.3190 micro-F1=0.6063 macro-F1=0.4748\n",
            " -> saved best model for fold 1  (micro-F1= 0.6062717770034843 )\n",
            "Fold 1 Epoch 17: loss=0.3210 micro-F1=0.5920 macro-F1=0.4599\n",
            "Fold 1 Epoch 18: loss=0.2920 micro-F1=0.6079 macro-F1=0.4744\n",
            " -> saved best model for fold 1  (micro-F1= 0.6079447322970639 )\n",
            "Fold 1 Epoch 19: loss=0.2428 micro-F1=0.6092 macro-F1=0.4797\n",
            " -> saved best model for fold 1  (micro-F1= 0.6091549295774648 )\n",
            "Fold 1 Epoch 20: loss=0.2406 micro-F1=0.6093 macro-F1=0.4953\n",
            " -> saved best model for fold 1  (micro-F1= 0.6092943201376936 )\n",
            "Fold 1 done. Best micro-F1: 0.6092943201376936\n",
            "\n",
            "=== FOLD 2 / 5 ===\n",
            "Fold 2 Epoch 1: loss=1.0217 micro-F1=0.4968 macro-F1=0.4129\n",
            " -> saved best model for fold 2  (micro-F1= 0.4967989756722151 )\n",
            "Fold 2 Epoch 2: loss=0.7509 micro-F1=0.5537 macro-F1=0.4586\n",
            " -> saved best model for fold 2  (micro-F1= 0.5536723163841808 )\n",
            "Fold 2 Epoch 3: loss=0.7206 micro-F1=0.6024 macro-F1=0.4972\n",
            " -> saved best model for fold 2  (micro-F1= 0.6024464831804281 )\n",
            "Fold 2 Epoch 4: loss=0.6307 micro-F1=0.6080 macro-F1=0.4932\n",
            " -> saved best model for fold 2  (micro-F1= 0.6080246913580247 )\n",
            "Fold 2 Epoch 5: loss=0.6051 micro-F1=0.6117 macro-F1=0.4988\n",
            " -> saved best model for fold 2  (micro-F1= 0.6116941529235382 )\n",
            "Fold 2 Epoch 6: loss=0.5878 micro-F1=0.6220 macro-F1=0.4941\n",
            " -> saved best model for fold 2  (micro-F1= 0.6220095693779905 )\n",
            "Fold 2 Epoch 7: loss=0.5627 micro-F1=0.6236 macro-F1=0.5017\n",
            " -> saved best model for fold 2  (micro-F1= 0.6236220472440945 )\n",
            "Fold 2 Epoch 8: loss=0.5612 micro-F1=0.6316 macro-F1=0.5126\n",
            " -> saved best model for fold 2  (micro-F1= 0.631578947368421 )\n",
            "Fold 2 Epoch 9: loss=0.5352 micro-F1=0.6264 macro-F1=0.4967\n",
            "Fold 2 Epoch 10: loss=0.5102 micro-F1=0.6667 macro-F1=0.5372\n",
            " -> saved best model for fold 2  (micro-F1= 0.6666666666666666 )\n",
            "Fold 2 Epoch 11: loss=0.4835 micro-F1=0.6587 macro-F1=0.5291\n",
            "Fold 2 Epoch 12: loss=0.4766 micro-F1=0.6477 macro-F1=0.5282\n",
            "Fold 2 Epoch 13: loss=0.4387 micro-F1=0.6838 macro-F1=0.5479\n",
            " -> saved best model for fold 2  (micro-F1= 0.6838487972508591 )\n",
            "Fold 2 Epoch 14: loss=0.4752 micro-F1=0.6746 macro-F1=0.5445\n",
            "Fold 2 Epoch 15: loss=0.4581 micro-F1=0.6678 macro-F1=0.5464\n",
            "Fold 2 Epoch 16: loss=0.3846 micro-F1=0.6678 macro-F1=0.5329\n",
            "Fold 2 Epoch 17: loss=0.3945 micro-F1=0.6714 macro-F1=0.5341\n",
            "Fold 2 Epoch 18: loss=0.3668 micro-F1=0.6961 macro-F1=0.5685\n",
            " -> saved best model for fold 2  (micro-F1= 0.696113074204947 )\n",
            "Fold 2 Epoch 19: loss=0.3118 micro-F1=0.6941 macro-F1=0.5419\n",
            "Fold 2 Epoch 20: loss=0.3097 micro-F1=0.6950 macro-F1=0.5473\n",
            "Fold 2 done. Best micro-F1: 0.696113074204947\n",
            "\n",
            "=== FOLD 3 / 5 ===\n",
            "Fold 3 Epoch 1: loss=1.0159 micro-F1=0.4610 macro-F1=0.4130\n",
            " -> saved best model for fold 3  (micro-F1= 0.4610011641443539 )\n",
            "Fold 3 Epoch 2: loss=0.7506 micro-F1=0.5581 macro-F1=0.4745\n",
            " -> saved best model for fold 3  (micro-F1= 0.5580736543909348 )\n",
            "Fold 3 Epoch 3: loss=0.6871 micro-F1=0.5948 macro-F1=0.5126\n",
            " -> saved best model for fold 3  (micro-F1= 0.5947521865889213 )\n",
            "Fold 3 Epoch 4: loss=0.6024 micro-F1=0.6163 macro-F1=0.5151\n",
            " -> saved best model for fold 3  (micro-F1= 0.6162962962962963 )\n",
            "Fold 3 Epoch 5: loss=0.5946 micro-F1=0.5927 macro-F1=0.4993\n",
            "Fold 3 Epoch 6: loss=0.5545 micro-F1=0.6006 macro-F1=0.5075\n",
            "Fold 3 Epoch 7: loss=0.5569 micro-F1=0.6096 macro-F1=0.5104\n",
            "Fold 3 Epoch 8: loss=0.5510 micro-F1=0.6265 macro-F1=0.5241\n",
            " -> saved best model for fold 3  (micro-F1= 0.6265432098765432 )\n",
            "Fold 3 Epoch 9: loss=0.5103 micro-F1=0.6154 macro-F1=0.5142\n",
            "Fold 3 Epoch 10: loss=0.4812 micro-F1=0.6286 macro-F1=0.5217\n",
            " -> saved best model for fold 3  (micro-F1= 0.6285714285714286 )\n",
            "Fold 3 Epoch 11: loss=0.4655 micro-F1=0.6244 macro-F1=0.5212\n",
            "Fold 3 Epoch 12: loss=0.4753 micro-F1=0.6547 macro-F1=0.5416\n",
            " -> saved best model for fold 3  (micro-F1= 0.6546644844517185 )\n",
            "Fold 3 Epoch 13: loss=0.4351 micro-F1=0.7024 macro-F1=0.5817\n",
            " -> saved best model for fold 3  (micro-F1= 0.7024221453287197 )\n",
            "Fold 3 Epoch 14: loss=0.3994 micro-F1=0.6610 macro-F1=0.5576\n",
            "Fold 3 Epoch 15: loss=0.4063 micro-F1=0.6590 macro-F1=0.5407\n",
            "Fold 3 Epoch 16: loss=0.3596 micro-F1=0.6644 macro-F1=0.5665\n",
            "Fold 3 Epoch 17: loss=0.3476 micro-F1=0.6993 macro-F1=0.5827\n",
            "Fold 3 Epoch 18: loss=0.3358 micro-F1=0.7025 macro-F1=0.5881\n",
            " -> saved best model for fold 3  (micro-F1= 0.7025089605734767 )\n",
            "Fold 3 Epoch 19: loss=0.3092 micro-F1=0.6996 macro-F1=0.5919\n",
            "Fold 3 Epoch 20: loss=0.2866 micro-F1=0.7007 macro-F1=0.5794\n",
            "Fold 3 done. Best micro-F1: 0.7025089605734767\n",
            "\n",
            "=== FOLD 4 / 5 ===\n",
            "Fold 4 Epoch 1: loss=1.0359 micro-F1=0.4504 macro-F1=0.3837\n",
            " -> saved best model for fold 4  (micro-F1= 0.45040840140023336 )\n",
            "Fold 4 Epoch 2: loss=0.7826 micro-F1=0.5596 macro-F1=0.4309\n",
            " -> saved best model for fold 4  (micro-F1= 0.5596330275229358 )\n",
            "Fold 4 Epoch 3: loss=0.7003 micro-F1=0.5604 macro-F1=0.4358\n",
            " -> saved best model for fold 4  (micro-F1= 0.5603576751117735 )\n",
            "Fold 4 Epoch 4: loss=0.6172 micro-F1=0.6116 macro-F1=0.4969\n",
            " -> saved best model for fold 4  (micro-F1= 0.6115702479338843 )\n",
            "Fold 4 Epoch 5: loss=0.5940 micro-F1=0.6059 macro-F1=0.4822\n",
            "Fold 4 Epoch 6: loss=0.5751 micro-F1=0.5966 macro-F1=0.4724\n",
            "Fold 4 Epoch 7: loss=0.5762 micro-F1=0.6022 macro-F1=0.4953\n",
            "Fold 4 Epoch 8: loss=0.5547 micro-F1=0.5888 macro-F1=0.4731\n",
            "Fold 4 Epoch 9: loss=0.5426 micro-F1=0.5845 macro-F1=0.4807\n",
            "Fold 4 Epoch 10: loss=0.5236 micro-F1=0.6108 macro-F1=0.5037\n",
            "Fold 4 Epoch 11: loss=0.5314 micro-F1=0.6429 macro-F1=0.5384\n",
            " -> saved best model for fold 4  (micro-F1= 0.6428571428571429 )\n",
            "Fold 4 Epoch 12: loss=0.4761 micro-F1=0.6359 macro-F1=0.5195\n",
            "Fold 4 Epoch 13: loss=0.4677 micro-F1=0.6309 macro-F1=0.5072\n",
            "Fold 4 Epoch 14: loss=0.4361 micro-F1=0.6393 macro-F1=0.5082\n",
            "Fold 4 Epoch 15: loss=0.4179 micro-F1=0.6474 macro-F1=0.5430\n",
            " -> saved best model for fold 4  (micro-F1= 0.6473594548551959 )\n",
            "Fold 4 Epoch 16: loss=0.4099 micro-F1=0.6543 macro-F1=0.5399\n",
            " -> saved best model for fold 4  (micro-F1= 0.654300168634064 )\n",
            "Fold 4 Epoch 17: loss=0.3558 micro-F1=0.6549 macro-F1=0.5352\n",
            " -> saved best model for fold 4  (micro-F1= 0.6549295774647887 )\n",
            "Fold 4 Epoch 18: loss=0.3313 micro-F1=0.6476 macro-F1=0.5095\n",
            "Fold 4 Epoch 19: loss=0.3025 micro-F1=0.6596 macro-F1=0.5420\n",
            " -> saved best model for fold 4  (micro-F1= 0.6595744680851063 )\n",
            "Fold 4 Epoch 20: loss=0.2756 micro-F1=0.6595 macro-F1=0.5286\n",
            "Fold 4 done. Best micro-F1: 0.6595744680851063\n",
            "\n",
            "=== FOLD 5 / 5 ===\n",
            "Fold 5 Epoch 1: loss=1.0182 micro-F1=0.5014 macro-F1=0.4061\n",
            " -> saved best model for fold 5  (micro-F1= 0.5013550135501355 )\n",
            "Fold 5 Epoch 2: loss=0.7497 micro-F1=0.5549 macro-F1=0.4641\n",
            " -> saved best model for fold 5  (micro-F1= 0.5548961424332344 )\n",
            "Fold 5 Epoch 3: loss=0.6683 micro-F1=0.5551 macro-F1=0.4696\n",
            " -> saved best model for fold 5  (micro-F1= 0.555052790346908 )\n",
            "Fold 5 Epoch 4: loss=0.6254 micro-F1=0.5427 macro-F1=0.4499\n",
            "Fold 5 Epoch 5: loss=0.5949 micro-F1=0.5684 macro-F1=0.4673\n",
            " -> saved best model for fold 5  (micro-F1= 0.5683563748079877 )\n",
            "Fold 5 Epoch 6: loss=0.5753 micro-F1=0.5654 macro-F1=0.4723\n",
            "Fold 5 Epoch 7: loss=0.5786 micro-F1=0.5951 macro-F1=0.4829\n",
            " -> saved best model for fold 5  (micro-F1= 0.5951219512195122 )\n",
            "Fold 5 Epoch 8: loss=0.5390 micro-F1=0.5944 macro-F1=0.4930\n",
            "Fold 5 Epoch 9: loss=0.5206 micro-F1=0.5972 macro-F1=0.4876\n",
            " -> saved best model for fold 5  (micro-F1= 0.5972006220839814 )\n",
            "Fold 5 Epoch 10: loss=0.4851 micro-F1=0.6323 macro-F1=0.5056\n",
            " -> saved best model for fold 5  (micro-F1= 0.632279534109817 )\n",
            "Fold 5 Epoch 11: loss=0.4642 micro-F1=0.6112 macro-F1=0.5071\n",
            "Fold 5 Epoch 12: loss=0.4146 micro-F1=0.6326 macro-F1=0.5321\n",
            " -> saved best model for fold 5  (micro-F1= 0.6326194398682042 )\n",
            "Fold 5 Epoch 13: loss=0.3977 micro-F1=0.6425 macro-F1=0.5280\n",
            " -> saved best model for fold 5  (micro-F1= 0.6424870466321243 )\n",
            "Fold 5 Epoch 14: loss=0.4434 micro-F1=0.6424 macro-F1=0.5246\n",
            "Fold 5 Epoch 15: loss=0.3713 micro-F1=0.6490 macro-F1=0.5430\n",
            " -> saved best model for fold 5  (micro-F1= 0.6490299823633157 )\n",
            "Fold 5 Epoch 16: loss=0.3505 micro-F1=0.6620 macro-F1=0.5446\n",
            " -> saved best model for fold 5  (micro-F1= 0.6619964973730298 )\n",
            "Fold 5 Epoch 17: loss=0.3172 micro-F1=0.6895 macro-F1=0.5880\n",
            " -> saved best model for fold 5  (micro-F1= 0.6895306859205776 )\n",
            "Fold 5 Epoch 18: loss=0.3173 micro-F1=0.6752 macro-F1=0.5762\n",
            "Fold 5 Epoch 19: loss=0.2819 micro-F1=0.6902 macro-F1=0.5536\n",
            " -> saved best model for fold 5  (micro-F1= 0.6901669758812616 )\n",
            "Fold 5 Epoch 20: loss=0.2676 micro-F1=0.6803 macro-F1=0.5488\n",
            "Fold 5 done. Best micro-F1: 0.6901669758812616\n",
            "\n",
            "Best global threshold: 0.75 | micro-F1: 0.6911 | macro-F1: 0.5662\n",
            "Saved OOF logits to /content/image_audio_oof_logits.npy\n",
            "Updated summary CSV at /content/analysis_outputs/summary_metrics.csv\n",
            "Saved /content/y_true.npy and /content/fold_ids.npy (if replaced).\n",
            "Done (Image + Audio).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell: compute bootstrap 95% CIs from saved *_oof_logits.npy and y_true.npy\n",
        "# Saves: /content/analysis_outputs/summary_with_CIs.csv\n",
        "\n",
        "!pip install -q scikit-learn numpy pandas tqdm\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from math import isfinite\n",
        "\n",
        "OUT_DIR = \"/content/analysis_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# list of model oof logits to check (update names if yours differ)\n",
        "model_files = {\n",
        "    \"tri-modal\": \"/content/trimodal_oof_logits.npy\",\n",
        "    \"image-only\": \"/content/image_only_oof_logits.npy\",\n",
        "    \"text-only\": \"/content/text_only_oof_logits.npy\",\n",
        "    \"audio-only\": \"/content/audio_only_oof_logits.npy\",\n",
        "    \"image+audio\": \"/content/image_audio_oof_logits.npy\",\n",
        "    \"text+audio\": \"/content/text_audio_oof_logits.npy\",\n",
        "    \"text+image\": \"/content/text_image_oof_logits.npy\"\n",
        "}\n",
        "\n",
        "# load y_true\n",
        "if not os.path.exists(\"/content/y_true.npy\"):\n",
        "    raise FileNotFoundError(\"Please make sure /content/y_true.npy exists (multi-hot matrix).\")\n",
        "y_true = np.load(\"/content/y_true.npy\")  # shape (N, C)\n",
        "N, C = y_true.shape\n",
        "print(\"Loaded y_true:\", y_true.shape)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ensure_probs(arr):\n",
        "    # if already in [0,1], assume probabilities; otherwise apply sigmoid\n",
        "    if np.nanmin(arr) >= -1e-6 and np.nanmax(arr) <= 1.000001:\n",
        "        return arr.astype(np.float32)\n",
        "    return sigmoid(arr)\n",
        "\n",
        "def best_threshold_and_scores(y_true, y_prob, thr_grid=None):\n",
        "    if thr_grid is None:\n",
        "        thr_grid = np.linspace(0.30, 0.80, 51)\n",
        "    best_thr = thr_grid[0]\n",
        "    best_micro = -1.0\n",
        "    best_macro = -1.0\n",
        "    for thr in thr_grid:\n",
        "        y_pred = (y_prob >= thr).astype(int)\n",
        "        micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "        if micro > best_micro:\n",
        "            best_micro = micro\n",
        "            best_macro = macro\n",
        "            best_thr = thr\n",
        "    return best_thr, best_micro, best_macro\n",
        "\n",
        "def bootstrap_ci(y_true, y_prob, thr, n_boot=1000, random_seed=42):\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    micro_scores = []\n",
        "    macro_scores = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, N, size=N)  # sample with replacement\n",
        "        y_t = y_true[idx]\n",
        "        y_p = y_prob[idx]\n",
        "        y_pred = (y_p >= thr).astype(int)\n",
        "        micro = f1_score(y_t, y_pred, average=\"micro\", zero_division=0)\n",
        "        macro = f1_score(y_t, y_pred, average=\"macro\", zero_division=0)\n",
        "        micro_scores.append(micro)\n",
        "        macro_scores.append(macro)\n",
        "    micro_scores = np.array(micro_scores)\n",
        "    macro_scores = np.array(macro_scores)\n",
        "    # 95% CI: 2.5 and 97.5 percentiles\n",
        "    return {\n",
        "        \"micro_mean\": float(np.mean(micro_scores)),\n",
        "        \"micro_ci_low\": float(np.percentile(micro_scores, 2.5)),\n",
        "        \"micro_ci_high\": float(np.percentile(micro_scores, 97.5)),\n",
        "        \"macro_mean\": float(np.mean(macro_scores)),\n",
        "        \"macro_ci_low\": float(np.percentile(macro_scores, 2.5)),\n",
        "        \"macro_ci_high\": float(np.percentile(macro_scores, 97.5)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "for model_name, path in model_files.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"SKIP (missing): {model_name} -> {path}\")\n",
        "        continue\n",
        "    logits = np.load(path)  # can be logits or probabilities\n",
        "    if logits.shape != (N, C):\n",
        "        print(f\"ERROR shape mismatch for {model_name}: logits {logits.shape} vs y_true {y_true.shape} -- SKIPPING\")\n",
        "        continue\n",
        "    probs = ensure_probs(logits)\n",
        "    # check values valid\n",
        "    if not (np.isfinite(probs).all()):\n",
        "        print(f\"WARNING: non-finite values in {model_name}, replacing with 0.0\")\n",
        "        probs = np.nan_to_num(probs, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "\n",
        "    # Find best threshold on full set (same as earlier procedure)\n",
        "    best_thr, best_micro, best_macro = best_threshold_and_scores(y_true, probs, thr_grid=np.linspace(0.30, 0.80, 51))\n",
        "    print(f\"{model_name}: best_thr={best_thr:.2f} micro={best_micro:.4f} macro={best_macro:.4f}\")\n",
        "\n",
        "    # bootstrap CI (n_boot=1000 default — change to 5000 if you want tighter CI but slower)\n",
        "    stats = bootstrap_ci(y_true, probs, thr=best_thr, n_boot=1000, random_seed=42)\n",
        "    row = {\n",
        "        \"model\": model_name,\n",
        "        \"micro_f1\": best_micro,\n",
        "        \"micro_ci_low\": stats[\"micro_ci_low\"],\n",
        "        \"micro_ci_high\": stats[\"micro_ci_high\"],\n",
        "        \"macro_f1\": best_macro,\n",
        "        \"macro_ci_low\": stats[\"macro_ci_low\"],\n",
        "        \"macro_ci_high\": stats[\"macro_ci_high\"],\n",
        "        \"threshold\": best_thr\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "# save summary CSV\n",
        "df_sum = pd.DataFrame(rows)\n",
        "df_sum = df_sum.sort_values(\"micro_f1\", ascending=False).reset_index(drop=True)\n",
        "out_csv = os.path.join(OUT_DIR, \"summary_with_CIs.csv\")\n",
        "df_sum.to_csv(out_csv, index=False)\n",
        "print(\"Saved summary with CIs to:\", out_csv)\n",
        "print(df_sum.to_string(index=False, float_format='%.4f'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jbz_N9N89oN",
        "outputId": "617badd4-b2f5-4e13-ef42-8f7237a1d4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded y_true: (404, 18)\n",
            "tri-modal: best_thr=0.50 micro=0.7822 macro=0.6786\n",
            "image-only: best_thr=0.79 micro=0.6278 macro=0.4994\n",
            "text-only: best_thr=0.51 micro=0.7085 macro=0.4929\n",
            "audio-only: best_thr=0.58 micro=0.5004 macro=0.3230\n",
            "image+audio: best_thr=0.77 micro=0.6914 macro=0.5656\n",
            "text+audio: best_thr=0.50 micro=0.7082 macro=0.5776\n",
            "text+image: best_thr=0.80 micro=0.6768 macro=0.5443\n",
            "Saved summary with CIs to: /content/analysis_outputs/summary_with_CIs.csv\n",
            "      model  micro_f1  micro_ci_low  micro_ci_high  macro_f1  macro_ci_low  macro_ci_high  threshold\n",
            "  tri-modal    0.7822        0.7514         0.8125    0.6786        0.6315         0.7172     0.5000\n",
            "  text-only    0.7085        0.6801         0.7425    0.4929        0.4453         0.5353     0.5100\n",
            " text+audio    0.7082        0.6785         0.7404    0.5776        0.5327         0.6147     0.5000\n",
            "image+audio    0.6914        0.6518         0.7278    0.5656        0.5207         0.6033     0.7700\n",
            " text+image    0.6768        0.6359         0.7163    0.5443        0.4966         0.5858     0.8000\n",
            " image-only    0.6278        0.5856         0.6658    0.4994        0.4570         0.5379     0.7900\n",
            " audio-only    0.5004        0.4661         0.5355    0.3230        0.2879         0.3515     0.5800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell: plot horizontal bar chart for Micro-F1 and Macro-F1 with 95% CIs\n",
        "# Reads: /content/analysis_outputs/summary_with_CIs.csv\n",
        "# Saves: /content/analysis_outputs/f1_comparison.png\n",
        "\n",
        "!pip install -q matplotlib pandas numpy\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CSV = \"/content/analysis_outputs/summary_with_CIs.csv\"\n",
        "if not os.path.exists(CSV):\n",
        "    raise FileNotFoundError(f\"{CSV} not found. Run the bootstrap CI cell first.\")\n",
        "\n",
        "df = pd.read_csv(CSV)\n",
        "# order by micro_f1 descending for plotting\n",
        "df = df.sort_values(\"micro_f1\", ascending=True)  # ascending for horizontal bars (bottom-up)\n",
        "\n",
        "models = df[\"model\"].tolist()\n",
        "y_pos = np.arange(len(models))\n",
        "\n",
        "# micro & macro and their error extents\n",
        "micro = df[\"micro_f1\"].values\n",
        "micro_err_low = micro - df[\"micro_ci_low\"].values\n",
        "micro_err_high = df[\"micro_ci_high\"].values - micro\n",
        "micro_err = np.vstack([micro_err_low, micro_err_high])\n",
        "\n",
        "macro = df[\"macro_f1\"].values\n",
        "macro_err_low = macro - df[\"macro_ci_low\"].values\n",
        "macro_err_high = df[\"macro_ci_high\"].values - macro\n",
        "macro_err = np.vstack([macro_err_low, macro_err_high])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8.5, max(4, 0.5*len(models))))\n",
        "bar_height = 0.35\n",
        "\n",
        "# plot micro bars\n",
        "ax.barh(y_pos - bar_height/2, micro, height=bar_height, xerr=micro_err, align='center',\n",
        "        label='Micro-F1', ecolor='black', capsize=4)\n",
        "\n",
        "# plot macro bars\n",
        "ax.barh(y_pos + bar_height/2, macro, height=bar_height, xerr=macro_err, align='center',\n",
        "        label='Macro-F1', ecolor='black', capsize=4)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(models)\n",
        "ax.set_xlabel(\"F1 score\")\n",
        "ax.set_xlim(0.0, 1.0)\n",
        "ax.set_title(\"Model comparison — Micro-F1 and Macro-F1 (95% CI)\")\n",
        "ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "out_png = \"/content/analysis_outputs/f1_comparison.png\"\n",
        "plt.savefig(out_png, dpi=300, bbox_inches='tight')\n",
        "print(\"Saved figure to\", out_png)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "zY7ZlVkm_XnO",
        "outputId": "59fe795e-7b07-4b62-b7c5-2b168c7ddbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved figure to /content/analysis_outputs/f1_comparison.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 850x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}